{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VwDJShqnp7e-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('/content/auto-mpg.data', delim_whitespace=True, header=None, na_values='?')\n",
        "data.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']\n",
        "data = data.dropna()  # Drop rows with missing values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QhQnqjcsDfQ",
        "outputId": "d4d550c7-4905-4ca3-adc3-66ae15812077"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9f754a276b28>:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  data = pd.read_csv('/content/auto-mpg.data', delim_whitespace=True, header=None, na_values='?')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and target\n",
        "X = data[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin']].values\n",
        "y = data['mpg'].values  # Target variable is 'mpg'\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vCZwCzDbsE2N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom Dataset class for PyTorch\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "qlkVNO4TsHPo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model class\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
        "        super(MLPModel, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        for i, hidden_size in enumerate(hidden_layers):\n",
        "            layers.append(nn.Linear(input_size if i == 0 else hidden_layers[i-1], hidden_size))\n",
        "            if activation_fn == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation_fn == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation_fn == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "\n",
        "        layers.append(nn.Linear(hidden_layers[-1], 1))  # Output layer for regression\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Y3HkR69QsJM2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to train and evaluate the model\n",
        "def train_and_evaluate(hidden_layers, activation_fn, learning_rate, batch_size, epochs):\n",
        "    # Initialize dataset and dataloaders\n",
        "    train_dataset = CustomDataset(X_train, y_train)\n",
        "    test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MLPModel(input_size=X_train.shape[1], hidden_layers=hidden_layers, activation_fn=activation_fn).to(device)\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")  # Verbose output\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "43fGfCuwsLan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Comparing Hidden Layers and Neurons\n",
        "hidden_layers_options = [[4], [8], [16], [32], [64], [128]]\n",
        "results_hidden_layers = []\n",
        "for hidden_layers in hidden_layers_options:\n",
        "    avg_loss = train_and_evaluate(hidden_layers, activation_fn='relu', learning_rate=0.01, batch_size=32, epochs=50)\n",
        "    results_hidden_layers.append({'hidden_layers': hidden_layers, 'avg_loss': avg_loss})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya8ihulFsOQo",
        "outputId": "a1afa8ad-fa21-455e-95d2-07b8da3edd8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 602.3215\n",
            "Epoch [2/50], Loss: 593.0238\n",
            "Epoch [3/50], Loss: 576.6078\n",
            "Epoch [4/50], Loss: 557.5342\n",
            "Epoch [5/50], Loss: 529.6365\n",
            "Epoch [6/50], Loss: 496.5466\n",
            "Epoch [7/50], Loss: 453.2599\n",
            "Epoch [8/50], Loss: 399.6771\n",
            "Epoch [9/50], Loss: 337.6751\n",
            "Epoch [10/50], Loss: 267.0081\n",
            "Epoch [11/50], Loss: 198.4614\n",
            "Epoch [12/50], Loss: 139.4683\n",
            "Epoch [13/50], Loss: 94.2440\n",
            "Epoch [14/50], Loss: 68.0420\n",
            "Epoch [15/50], Loss: 53.1720\n",
            "Epoch [16/50], Loss: 45.8798\n",
            "Epoch [17/50], Loss: 42.0495\n",
            "Epoch [18/50], Loss: 38.7817\n",
            "Epoch [19/50], Loss: 36.1266\n",
            "Epoch [20/50], Loss: 33.2710\n",
            "Epoch [21/50], Loss: 30.7407\n",
            "Epoch [22/50], Loss: 28.3773\n",
            "Epoch [23/50], Loss: 26.2974\n",
            "Epoch [24/50], Loss: 24.4697\n",
            "Epoch [25/50], Loss: 22.7032\n",
            "Epoch [26/50], Loss: 21.4477\n",
            "Epoch [27/50], Loss: 19.8935\n",
            "Epoch [28/50], Loss: 18.8726\n",
            "Epoch [29/50], Loss: 17.7221\n",
            "Epoch [30/50], Loss: 16.8087\n",
            "Epoch [31/50], Loss: 16.0793\n",
            "Epoch [32/50], Loss: 15.5460\n",
            "Epoch [33/50], Loss: 14.5577\n",
            "Epoch [34/50], Loss: 13.8341\n",
            "Epoch [35/50], Loss: 13.3549\n",
            "Epoch [36/50], Loss: 12.8080\n",
            "Epoch [37/50], Loss: 12.3430\n",
            "Epoch [38/50], Loss: 12.2387\n",
            "Epoch [39/50], Loss: 12.1335\n",
            "Epoch [40/50], Loss: 11.5379\n",
            "Epoch [41/50], Loss: 11.2833\n",
            "Epoch [42/50], Loss: 11.3805\n",
            "Epoch [43/50], Loss: 11.0507\n",
            "Epoch [44/50], Loss: 10.9077\n",
            "Epoch [45/50], Loss: 11.1532\n",
            "Epoch [46/50], Loss: 10.8585\n",
            "Epoch [47/50], Loss: 10.7671\n",
            "Epoch [48/50], Loss: 10.5174\n",
            "Epoch [49/50], Loss: 10.4932\n",
            "Epoch [50/50], Loss: 10.4244\n",
            "Epoch [1/50], Loss: 612.4649\n",
            "Epoch [2/50], Loss: 578.1351\n",
            "Epoch [3/50], Loss: 529.3554\n",
            "Epoch [4/50], Loss: 463.5603\n",
            "Epoch [5/50], Loss: 383.1332\n",
            "Epoch [6/50], Loss: 292.1513\n",
            "Epoch [7/50], Loss: 205.0815\n",
            "Epoch [8/50], Loss: 135.9350\n",
            "Epoch [9/50], Loss: 92.9610\n",
            "Epoch [10/50], Loss: 69.5183\n",
            "Epoch [11/50], Loss: 57.4489\n",
            "Epoch [12/50], Loss: 49.5590\n",
            "Epoch [13/50], Loss: 42.8546\n",
            "Epoch [14/50], Loss: 37.1169\n",
            "Epoch [15/50], Loss: 32.7161\n",
            "Epoch [16/50], Loss: 29.2487\n",
            "Epoch [17/50], Loss: 26.0653\n",
            "Epoch [18/50], Loss: 23.8219\n",
            "Epoch [19/50], Loss: 22.2644\n",
            "Epoch [20/50], Loss: 20.5304\n",
            "Epoch [21/50], Loss: 19.3574\n",
            "Epoch [22/50], Loss: 18.5691\n",
            "Epoch [23/50], Loss: 18.4071\n",
            "Epoch [24/50], Loss: 17.0096\n",
            "Epoch [25/50], Loss: 16.5271\n",
            "Epoch [26/50], Loss: 15.9840\n",
            "Epoch [27/50], Loss: 15.5735\n",
            "Epoch [28/50], Loss: 15.0767\n",
            "Epoch [29/50], Loss: 14.8155\n",
            "Epoch [30/50], Loss: 14.3979\n",
            "Epoch [31/50], Loss: 13.8664\n",
            "Epoch [32/50], Loss: 13.6320\n",
            "Epoch [33/50], Loss: 13.2067\n",
            "Epoch [34/50], Loss: 13.0621\n",
            "Epoch [35/50], Loss: 12.8518\n",
            "Epoch [36/50], Loss: 12.6858\n",
            "Epoch [37/50], Loss: 12.3032\n",
            "Epoch [38/50], Loss: 12.0963\n",
            "Epoch [39/50], Loss: 11.9928\n",
            "Epoch [40/50], Loss: 11.6516\n",
            "Epoch [41/50], Loss: 11.5799\n",
            "Epoch [42/50], Loss: 11.4412\n",
            "Epoch [43/50], Loss: 11.0593\n",
            "Epoch [44/50], Loss: 11.0220\n",
            "Epoch [45/50], Loss: 10.9029\n",
            "Epoch [46/50], Loss: 10.6387\n",
            "Epoch [47/50], Loss: 10.6533\n",
            "Epoch [48/50], Loss: 10.3698\n",
            "Epoch [49/50], Loss: 10.3319\n",
            "Epoch [50/50], Loss: 10.2205\n",
            "Epoch [1/50], Loss: 594.0308\n",
            "Epoch [2/50], Loss: 536.8074\n",
            "Epoch [3/50], Loss: 458.2991\n",
            "Epoch [4/50], Loss: 360.9982\n",
            "Epoch [5/50], Loss: 250.2521\n",
            "Epoch [6/50], Loss: 148.8806\n",
            "Epoch [7/50], Loss: 76.1510\n",
            "Epoch [8/50], Loss: 51.6454\n",
            "Epoch [9/50], Loss: 45.4533\n",
            "Epoch [10/50], Loss: 37.3856\n",
            "Epoch [11/50], Loss: 30.8221\n",
            "Epoch [12/50], Loss: 26.5039\n",
            "Epoch [13/50], Loss: 23.4161\n",
            "Epoch [14/50], Loss: 20.9695\n",
            "Epoch [15/50], Loss: 18.8365\n",
            "Epoch [16/50], Loss: 17.6723\n",
            "Epoch [17/50], Loss: 16.4561\n",
            "Epoch [18/50], Loss: 15.3148\n",
            "Epoch [19/50], Loss: 14.4751\n",
            "Epoch [20/50], Loss: 14.0768\n",
            "Epoch [21/50], Loss: 13.5626\n",
            "Epoch [22/50], Loss: 12.9859\n",
            "Epoch [23/50], Loss: 12.5837\n",
            "Epoch [24/50], Loss: 12.2287\n",
            "Epoch [25/50], Loss: 11.8067\n",
            "Epoch [26/50], Loss: 11.5670\n",
            "Epoch [27/50], Loss: 11.2517\n",
            "Epoch [28/50], Loss: 10.8721\n",
            "Epoch [29/50], Loss: 10.6569\n",
            "Epoch [30/50], Loss: 10.7516\n",
            "Epoch [31/50], Loss: 10.4471\n",
            "Epoch [32/50], Loss: 10.0575\n",
            "Epoch [33/50], Loss: 9.9443\n",
            "Epoch [34/50], Loss: 9.6960\n",
            "Epoch [35/50], Loss: 9.3849\n",
            "Epoch [36/50], Loss: 9.2817\n",
            "Epoch [37/50], Loss: 9.1710\n",
            "Epoch [38/50], Loss: 8.9615\n",
            "Epoch [39/50], Loss: 8.7913\n",
            "Epoch [40/50], Loss: 8.7624\n",
            "Epoch [41/50], Loss: 8.5871\n",
            "Epoch [42/50], Loss: 8.5577\n",
            "Epoch [43/50], Loss: 8.5588\n",
            "Epoch [44/50], Loss: 8.3132\n",
            "Epoch [45/50], Loss: 8.3616\n",
            "Epoch [46/50], Loss: 8.1496\n",
            "Epoch [47/50], Loss: 8.1627\n",
            "Epoch [48/50], Loss: 8.0907\n",
            "Epoch [49/50], Loss: 7.9990\n",
            "Epoch [50/50], Loss: 8.0615\n",
            "Epoch [1/50], Loss: 587.6083\n",
            "Epoch [2/50], Loss: 506.7071\n",
            "Epoch [3/50], Loss: 392.7591\n",
            "Epoch [4/50], Loss: 254.8133\n",
            "Epoch [5/50], Loss: 127.9787\n",
            "Epoch [6/50], Loss: 69.8267\n",
            "Epoch [7/50], Loss: 55.8967\n",
            "Epoch [8/50], Loss: 39.6214\n",
            "Epoch [9/50], Loss: 29.1056\n",
            "Epoch [10/50], Loss: 23.6619\n",
            "Epoch [11/50], Loss: 19.7914\n",
            "Epoch [12/50], Loss: 17.3725\n",
            "Epoch [13/50], Loss: 15.8904\n",
            "Epoch [14/50], Loss: 14.5356\n",
            "Epoch [15/50], Loss: 13.5971\n",
            "Epoch [16/50], Loss: 12.5538\n",
            "Epoch [17/50], Loss: 12.0286\n",
            "Epoch [18/50], Loss: 11.3647\n",
            "Epoch [19/50], Loss: 11.0119\n",
            "Epoch [20/50], Loss: 10.8592\n",
            "Epoch [21/50], Loss: 10.3836\n",
            "Epoch [22/50], Loss: 10.3612\n",
            "Epoch [23/50], Loss: 10.0127\n",
            "Epoch [24/50], Loss: 9.9128\n",
            "Epoch [25/50], Loss: 9.5795\n",
            "Epoch [26/50], Loss: 9.3458\n",
            "Epoch [27/50], Loss: 9.2038\n",
            "Epoch [28/50], Loss: 8.9854\n",
            "Epoch [29/50], Loss: 8.9663\n",
            "Epoch [30/50], Loss: 8.8523\n",
            "Epoch [31/50], Loss: 8.5230\n",
            "Epoch [32/50], Loss: 8.5644\n",
            "Epoch [33/50], Loss: 8.2992\n",
            "Epoch [34/50], Loss: 8.3565\n",
            "Epoch [35/50], Loss: 8.2603\n",
            "Epoch [36/50], Loss: 8.5176\n",
            "Epoch [37/50], Loss: 7.9873\n",
            "Epoch [38/50], Loss: 8.1892\n",
            "Epoch [39/50], Loss: 7.9150\n",
            "Epoch [40/50], Loss: 8.1718\n",
            "Epoch [41/50], Loss: 7.9768\n",
            "Epoch [42/50], Loss: 7.7561\n",
            "Epoch [43/50], Loss: 7.7112\n",
            "Epoch [44/50], Loss: 7.6576\n",
            "Epoch [45/50], Loss: 7.6050\n",
            "Epoch [46/50], Loss: 7.5924\n",
            "Epoch [47/50], Loss: 7.8513\n",
            "Epoch [48/50], Loss: 7.7704\n",
            "Epoch [49/50], Loss: 7.5094\n",
            "Epoch [50/50], Loss: 7.4595\n",
            "Epoch [1/50], Loss: 556.5718\n",
            "Epoch [2/50], Loss: 392.0004\n",
            "Epoch [3/50], Loss: 180.8284\n",
            "Epoch [4/50], Loss: 63.6937\n",
            "Epoch [5/50], Loss: 55.1842\n",
            "Epoch [6/50], Loss: 33.3321\n",
            "Epoch [7/50], Loss: 24.1838\n",
            "Epoch [8/50], Loss: 18.1916\n",
            "Epoch [9/50], Loss: 15.5655\n",
            "Epoch [10/50], Loss: 13.5910\n",
            "Epoch [11/50], Loss: 12.7669\n",
            "Epoch [12/50], Loss: 11.8995\n",
            "Epoch [13/50], Loss: 11.0439\n",
            "Epoch [14/50], Loss: 10.9913\n",
            "Epoch [15/50], Loss: 10.3837\n",
            "Epoch [16/50], Loss: 9.8786\n",
            "Epoch [17/50], Loss: 9.7022\n",
            "Epoch [18/50], Loss: 9.5721\n",
            "Epoch [19/50], Loss: 9.2016\n",
            "Epoch [20/50], Loss: 8.8064\n",
            "Epoch [21/50], Loss: 8.8219\n",
            "Epoch [22/50], Loss: 8.6143\n",
            "Epoch [23/50], Loss: 8.3641\n",
            "Epoch [24/50], Loss: 8.2836\n",
            "Epoch [25/50], Loss: 8.3564\n",
            "Epoch [26/50], Loss: 8.5337\n",
            "Epoch [27/50], Loss: 8.0028\n",
            "Epoch [28/50], Loss: 7.8074\n",
            "Epoch [29/50], Loss: 7.9292\n",
            "Epoch [30/50], Loss: 8.0973\n",
            "Epoch [31/50], Loss: 7.7127\n",
            "Epoch [32/50], Loss: 7.6513\n",
            "Epoch [33/50], Loss: 7.5693\n",
            "Epoch [34/50], Loss: 7.4331\n",
            "Epoch [35/50], Loss: 7.5913\n",
            "Epoch [36/50], Loss: 7.4980\n",
            "Epoch [37/50], Loss: 7.3807\n",
            "Epoch [38/50], Loss: 7.1960\n",
            "Epoch [39/50], Loss: 7.1524\n",
            "Epoch [40/50], Loss: 7.2019\n",
            "Epoch [41/50], Loss: 7.0176\n",
            "Epoch [42/50], Loss: 7.1484\n",
            "Epoch [43/50], Loss: 7.0402\n",
            "Epoch [44/50], Loss: 7.0494\n",
            "Epoch [45/50], Loss: 6.9944\n",
            "Epoch [46/50], Loss: 6.9184\n",
            "Epoch [47/50], Loss: 7.1865\n",
            "Epoch [48/50], Loss: 7.0161\n",
            "Epoch [49/50], Loss: 6.8978\n",
            "Epoch [50/50], Loss: 7.0634\n",
            "Epoch [1/50], Loss: 539.6655\n",
            "Epoch [2/50], Loss: 303.2381\n",
            "Epoch [3/50], Loss: 88.6717\n",
            "Epoch [4/50], Loss: 56.3352\n",
            "Epoch [5/50], Loss: 29.3012\n",
            "Epoch [6/50], Loss: 21.0525\n",
            "Epoch [7/50], Loss: 14.9010\n",
            "Epoch [8/50], Loss: 12.9687\n",
            "Epoch [9/50], Loss: 11.7550\n",
            "Epoch [10/50], Loss: 10.9372\n",
            "Epoch [11/50], Loss: 10.3052\n",
            "Epoch [12/50], Loss: 9.9133\n",
            "Epoch [13/50], Loss: 9.6175\n",
            "Epoch [14/50], Loss: 9.6240\n",
            "Epoch [15/50], Loss: 9.0394\n",
            "Epoch [16/50], Loss: 8.9142\n",
            "Epoch [17/50], Loss: 8.8747\n",
            "Epoch [18/50], Loss: 8.6944\n",
            "Epoch [19/50], Loss: 8.4016\n",
            "Epoch [20/50], Loss: 8.0845\n",
            "Epoch [21/50], Loss: 7.8114\n",
            "Epoch [22/50], Loss: 8.0106\n",
            "Epoch [23/50], Loss: 7.7313\n",
            "Epoch [24/50], Loss: 7.7971\n",
            "Epoch [25/50], Loss: 7.3914\n",
            "Epoch [26/50], Loss: 7.6633\n",
            "Epoch [27/50], Loss: 7.5991\n",
            "Epoch [28/50], Loss: 7.3937\n",
            "Epoch [29/50], Loss: 7.2669\n",
            "Epoch [30/50], Loss: 7.0399\n",
            "Epoch [31/50], Loss: 7.1131\n",
            "Epoch [32/50], Loss: 7.1667\n",
            "Epoch [33/50], Loss: 7.1827\n",
            "Epoch [34/50], Loss: 6.8175\n",
            "Epoch [35/50], Loss: 6.9471\n",
            "Epoch [36/50], Loss: 7.0192\n",
            "Epoch [37/50], Loss: 6.9531\n",
            "Epoch [38/50], Loss: 6.6054\n",
            "Epoch [39/50], Loss: 6.7717\n",
            "Epoch [40/50], Loss: 6.6251\n",
            "Epoch [41/50], Loss: 6.5063\n",
            "Epoch [42/50], Loss: 6.5528\n",
            "Epoch [43/50], Loss: 6.5407\n",
            "Epoch [44/50], Loss: 6.8141\n",
            "Epoch [45/50], Loss: 6.8235\n",
            "Epoch [46/50], Loss: 6.3684\n",
            "Epoch [47/50], Loss: 6.6223\n",
            "Epoch [48/50], Loss: 6.4931\n",
            "Epoch [49/50], Loss: 6.4646\n",
            "Epoch [50/50], Loss: 6.4459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Comparing Activation Functions\n",
        "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
        "results_activation_functions = []\n",
        "for activation_fn in activation_functions:\n",
        "    avg_loss = train_and_evaluate(hidden_layers=[32], activation_fn=activation_fn, learning_rate=0.01, batch_size=32, epochs=50)\n",
        "    results_activation_functions.append({'activation_function': activation_fn, 'avg_loss': avg_loss})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksEmEwq8sQMW",
        "outputId": "438d71fa-c0fe-4e91-c23d-d7e757cbc917"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 569.6687\n",
            "Epoch [2/50], Loss: 469.4566\n",
            "Epoch [3/50], Loss: 324.8467\n",
            "Epoch [4/50], Loss: 165.2133\n",
            "Epoch [5/50], Loss: 72.5006\n",
            "Epoch [6/50], Loss: 58.6537\n",
            "Epoch [7/50], Loss: 46.9060\n",
            "Epoch [8/50], Loss: 34.4834\n",
            "Epoch [9/50], Loss: 28.5074\n",
            "Epoch [10/50], Loss: 23.6472\n",
            "Epoch [11/50], Loss: 20.6384\n",
            "Epoch [12/50], Loss: 18.3703\n",
            "Epoch [13/50], Loss: 16.3573\n",
            "Epoch [14/50], Loss: 15.2044\n",
            "Epoch [15/50], Loss: 14.0456\n",
            "Epoch [16/50], Loss: 13.2766\n",
            "Epoch [17/50], Loss: 12.5357\n",
            "Epoch [18/50], Loss: 12.1740\n",
            "Epoch [19/50], Loss: 11.6121\n",
            "Epoch [20/50], Loss: 11.1769\n",
            "Epoch [21/50], Loss: 10.8482\n",
            "Epoch [22/50], Loss: 10.4911\n",
            "Epoch [23/50], Loss: 9.9290\n",
            "Epoch [24/50], Loss: 10.1010\n",
            "Epoch [25/50], Loss: 9.3393\n",
            "Epoch [26/50], Loss: 9.1523\n",
            "Epoch [27/50], Loss: 8.8849\n",
            "Epoch [28/50], Loss: 8.9220\n",
            "Epoch [29/50], Loss: 8.5871\n",
            "Epoch [30/50], Loss: 8.4514\n",
            "Epoch [31/50], Loss: 8.8513\n",
            "Epoch [32/50], Loss: 8.1745\n",
            "Epoch [33/50], Loss: 8.1649\n",
            "Epoch [34/50], Loss: 8.1810\n",
            "Epoch [35/50], Loss: 7.9600\n",
            "Epoch [36/50], Loss: 8.0593\n",
            "Epoch [37/50], Loss: 7.9380\n",
            "Epoch [38/50], Loss: 7.6986\n",
            "Epoch [39/50], Loss: 7.5647\n",
            "Epoch [40/50], Loss: 7.7598\n",
            "Epoch [41/50], Loss: 7.5425\n",
            "Epoch [42/50], Loss: 7.5047\n",
            "Epoch [43/50], Loss: 7.3770\n",
            "Epoch [44/50], Loss: 7.5662\n",
            "Epoch [45/50], Loss: 7.3433\n",
            "Epoch [46/50], Loss: 7.3440\n",
            "Epoch [47/50], Loss: 7.3590\n",
            "Epoch [48/50], Loss: 7.3752\n",
            "Epoch [49/50], Loss: 7.2585\n",
            "Epoch [50/50], Loss: 7.2700\n",
            "Epoch [1/50], Loss: 596.3561\n",
            "Epoch [2/50], Loss: 518.9132\n",
            "Epoch [3/50], Loss: 434.4699\n",
            "Epoch [4/50], Loss: 352.6672\n",
            "Epoch [5/50], Loss: 275.5797\n",
            "Epoch [6/50], Loss: 203.3950\n",
            "Epoch [7/50], Loss: 149.9580\n",
            "Epoch [8/50], Loss: 106.0697\n",
            "Epoch [9/50], Loss: 73.4050\n",
            "Epoch [10/50], Loss: 50.0075\n",
            "Epoch [11/50], Loss: 34.9340\n",
            "Epoch [12/50], Loss: 25.5413\n",
            "Epoch [13/50], Loss: 20.1783\n",
            "Epoch [14/50], Loss: 17.5165\n",
            "Epoch [15/50], Loss: 16.0123\n",
            "Epoch [16/50], Loss: 15.4619\n",
            "Epoch [17/50], Loss: 14.9107\n",
            "Epoch [18/50], Loss: 14.4631\n",
            "Epoch [19/50], Loss: 14.2956\n",
            "Epoch [20/50], Loss: 14.3878\n",
            "Epoch [21/50], Loss: 13.9737\n",
            "Epoch [22/50], Loss: 13.9144\n",
            "Epoch [23/50], Loss: 13.6155\n",
            "Epoch [24/50], Loss: 13.5174\n",
            "Epoch [25/50], Loss: 13.4998\n",
            "Epoch [26/50], Loss: 12.9569\n",
            "Epoch [27/50], Loss: 13.1007\n",
            "Epoch [28/50], Loss: 12.9406\n",
            "Epoch [29/50], Loss: 12.5114\n",
            "Epoch [30/50], Loss: 12.3706\n",
            "Epoch [31/50], Loss: 12.1420\n",
            "Epoch [32/50], Loss: 12.2858\n",
            "Epoch [33/50], Loss: 11.7066\n",
            "Epoch [34/50], Loss: 11.7420\n",
            "Epoch [35/50], Loss: 11.6609\n",
            "Epoch [36/50], Loss: 11.3542\n",
            "Epoch [37/50], Loss: 11.1191\n",
            "Epoch [38/50], Loss: 11.2596\n",
            "Epoch [39/50], Loss: 10.9002\n",
            "Epoch [40/50], Loss: 10.5929\n",
            "Epoch [41/50], Loss: 10.5591\n",
            "Epoch [42/50], Loss: 10.5204\n",
            "Epoch [43/50], Loss: 10.3497\n",
            "Epoch [44/50], Loss: 10.1329\n",
            "Epoch [45/50], Loss: 9.9460\n",
            "Epoch [46/50], Loss: 9.8307\n",
            "Epoch [47/50], Loss: 9.7373\n",
            "Epoch [48/50], Loss: 9.6426\n",
            "Epoch [49/50], Loss: 9.6860\n",
            "Epoch [50/50], Loss: 9.6608\n",
            "Epoch [1/50], Loss: 610.3908\n",
            "Epoch [2/50], Loss: 558.6173\n",
            "Epoch [3/50], Loss: 505.7048\n",
            "Epoch [4/50], Loss: 443.8397\n",
            "Epoch [5/50], Loss: 367.4283\n",
            "Epoch [6/50], Loss: 284.4877\n",
            "Epoch [7/50], Loss: 201.9743\n",
            "Epoch [8/50], Loss: 127.4228\n",
            "Epoch [9/50], Loss: 75.7025\n",
            "Epoch [10/50], Loss: 42.1758\n",
            "Epoch [11/50], Loss: 25.3829\n",
            "Epoch [12/50], Loss: 19.0483\n",
            "Epoch [13/50], Loss: 16.8163\n",
            "Epoch [14/50], Loss: 16.0706\n",
            "Epoch [15/50], Loss: 15.1960\n",
            "Epoch [16/50], Loss: 14.5561\n",
            "Epoch [17/50], Loss: 14.2136\n",
            "Epoch [18/50], Loss: 14.0327\n",
            "Epoch [19/50], Loss: 13.2826\n",
            "Epoch [20/50], Loss: 12.8468\n",
            "Epoch [21/50], Loss: 12.4722\n",
            "Epoch [22/50], Loss: 11.9885\n",
            "Epoch [23/50], Loss: 11.6933\n",
            "Epoch [24/50], Loss: 11.1418\n",
            "Epoch [25/50], Loss: 10.8958\n",
            "Epoch [26/50], Loss: 10.4013\n",
            "Epoch [27/50], Loss: 10.1433\n",
            "Epoch [28/50], Loss: 9.7828\n",
            "Epoch [29/50], Loss: 9.6100\n",
            "Epoch [30/50], Loss: 9.4213\n",
            "Epoch [31/50], Loss: 9.0389\n",
            "Epoch [32/50], Loss: 8.9771\n",
            "Epoch [33/50], Loss: 8.9127\n",
            "Epoch [34/50], Loss: 8.7492\n",
            "Epoch [35/50], Loss: 8.6992\n",
            "Epoch [36/50], Loss: 8.6865\n",
            "Epoch [37/50], Loss: 8.4219\n",
            "Epoch [38/50], Loss: 8.2718\n",
            "Epoch [39/50], Loss: 8.1066\n",
            "Epoch [40/50], Loss: 8.0726\n",
            "Epoch [41/50], Loss: 8.1984\n",
            "Epoch [42/50], Loss: 8.0554\n",
            "Epoch [43/50], Loss: 7.9516\n",
            "Epoch [44/50], Loss: 7.6552\n",
            "Epoch [45/50], Loss: 7.7705\n",
            "Epoch [46/50], Loss: 7.6630\n",
            "Epoch [47/50], Loss: 7.5305\n",
            "Epoch [48/50], Loss: 7.5068\n",
            "Epoch [49/50], Loss: 7.4386\n",
            "Epoch [50/50], Loss: 7.5069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: Comparing Epochs\n",
        "epochs_options = [1, 10, 25, 50, 100, 250]\n",
        "results_epochs = []\n",
        "for epochs in epochs_options:\n",
        "    avg_loss = train_and_evaluate(hidden_layers=[32], activation_fn='relu', learning_rate=0.01, batch_size=32, epochs=epochs)\n",
        "    results_epochs.append({'epochs': epochs, 'avg_loss': avg_loss})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO7Dj-AXsR8_",
        "outputId": "eedaebd4-d105-445c-b081-c442a3013ac3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 582.3509\n",
            "Epoch [1/10], Loss: 594.1282\n",
            "Epoch [2/10], Loss: 507.7552\n",
            "Epoch [3/10], Loss: 380.0792\n",
            "Epoch [4/10], Loss: 215.2326\n",
            "Epoch [5/10], Loss: 94.3584\n",
            "Epoch [6/10], Loss: 55.0258\n",
            "Epoch [7/10], Loss: 47.6670\n",
            "Epoch [8/10], Loss: 34.2744\n",
            "Epoch [9/10], Loss: 27.3889\n",
            "Epoch [10/10], Loss: 22.5883\n",
            "Epoch [1/25], Loss: 586.3988\n",
            "Epoch [2/25], Loss: 488.5611\n",
            "Epoch [3/25], Loss: 348.4142\n",
            "Epoch [4/25], Loss: 190.5861\n",
            "Epoch [5/25], Loss: 81.4382\n",
            "Epoch [6/25], Loss: 50.6783\n",
            "Epoch [7/25], Loss: 44.2249\n",
            "Epoch [8/25], Loss: 32.4966\n",
            "Epoch [9/25], Loss: 26.1709\n",
            "Epoch [10/25], Loss: 21.6760\n",
            "Epoch [11/25], Loss: 18.8227\n",
            "Epoch [12/25], Loss: 16.5929\n",
            "Epoch [13/25], Loss: 14.9596\n",
            "Epoch [14/25], Loss: 13.8392\n",
            "Epoch [15/25], Loss: 12.9900\n",
            "Epoch [16/25], Loss: 12.6021\n",
            "Epoch [17/25], Loss: 11.6503\n",
            "Epoch [18/25], Loss: 11.0914\n",
            "Epoch [19/25], Loss: 11.0200\n",
            "Epoch [20/25], Loss: 10.4973\n",
            "Epoch [21/25], Loss: 10.1117\n",
            "Epoch [22/25], Loss: 9.8555\n",
            "Epoch [23/25], Loss: 9.4880\n",
            "Epoch [24/25], Loss: 9.0814\n",
            "Epoch [25/25], Loss: 8.7396\n",
            "Epoch [1/50], Loss: 606.1870\n",
            "Epoch [2/50], Loss: 533.1994\n",
            "Epoch [3/50], Loss: 425.5110\n",
            "Epoch [4/50], Loss: 288.7254\n",
            "Epoch [5/50], Loss: 153.2734\n",
            "Epoch [6/50], Loss: 65.1128\n",
            "Epoch [7/50], Loss: 41.3594\n",
            "Epoch [8/50], Loss: 34.2467\n",
            "Epoch [9/50], Loss: 24.3141\n",
            "Epoch [10/50], Loss: 20.1658\n",
            "Epoch [11/50], Loss: 17.3104\n",
            "Epoch [12/50], Loss: 15.4429\n",
            "Epoch [13/50], Loss: 14.2494\n",
            "Epoch [14/50], Loss: 13.6199\n",
            "Epoch [15/50], Loss: 12.6358\n",
            "Epoch [16/50], Loss: 12.1605\n",
            "Epoch [17/50], Loss: 11.7813\n",
            "Epoch [18/50], Loss: 11.4328\n",
            "Epoch [19/50], Loss: 11.0016\n",
            "Epoch [20/50], Loss: 10.8478\n",
            "Epoch [21/50], Loss: 10.2927\n",
            "Epoch [22/50], Loss: 10.0232\n",
            "Epoch [23/50], Loss: 9.8738\n",
            "Epoch [24/50], Loss: 9.6355\n",
            "Epoch [25/50], Loss: 9.4765\n",
            "Epoch [26/50], Loss: 9.4860\n",
            "Epoch [27/50], Loss: 9.2696\n",
            "Epoch [28/50], Loss: 9.0511\n",
            "Epoch [29/50], Loss: 8.9267\n",
            "Epoch [30/50], Loss: 8.7684\n",
            "Epoch [31/50], Loss: 8.4663\n",
            "Epoch [32/50], Loss: 8.3839\n",
            "Epoch [33/50], Loss: 8.1738\n",
            "Epoch [34/50], Loss: 8.1288\n",
            "Epoch [35/50], Loss: 8.3457\n",
            "Epoch [36/50], Loss: 7.9952\n",
            "Epoch [37/50], Loss: 8.0565\n",
            "Epoch [38/50], Loss: 7.8946\n",
            "Epoch [39/50], Loss: 7.8543\n",
            "Epoch [40/50], Loss: 7.7885\n",
            "Epoch [41/50], Loss: 7.6692\n",
            "Epoch [42/50], Loss: 7.7729\n",
            "Epoch [43/50], Loss: 7.5543\n",
            "Epoch [44/50], Loss: 7.5579\n",
            "Epoch [45/50], Loss: 7.4866\n",
            "Epoch [46/50], Loss: 7.3774\n",
            "Epoch [47/50], Loss: 7.6136\n",
            "Epoch [48/50], Loss: 7.4463\n",
            "Epoch [49/50], Loss: 7.3128\n",
            "Epoch [50/50], Loss: 7.3091\n",
            "Epoch [1/100], Loss: 571.0132\n",
            "Epoch [2/100], Loss: 465.0271\n",
            "Epoch [3/100], Loss: 309.1407\n",
            "Epoch [4/100], Loss: 153.4003\n",
            "Epoch [5/100], Loss: 67.4405\n",
            "Epoch [6/100], Loss: 53.2804\n",
            "Epoch [7/100], Loss: 41.2958\n",
            "Epoch [8/100], Loss: 31.2308\n",
            "Epoch [9/100], Loss: 25.0560\n",
            "Epoch [10/100], Loss: 20.5698\n",
            "Epoch [11/100], Loss: 17.7086\n",
            "Epoch [12/100], Loss: 16.0546\n",
            "Epoch [13/100], Loss: 14.3676\n",
            "Epoch [14/100], Loss: 13.2872\n",
            "Epoch [15/100], Loss: 12.4411\n",
            "Epoch [16/100], Loss: 11.8653\n",
            "Epoch [17/100], Loss: 11.4090\n",
            "Epoch [18/100], Loss: 10.8938\n",
            "Epoch [19/100], Loss: 10.4363\n",
            "Epoch [20/100], Loss: 10.1312\n",
            "Epoch [21/100], Loss: 9.8084\n",
            "Epoch [22/100], Loss: 9.5900\n",
            "Epoch [23/100], Loss: 9.4010\n",
            "Epoch [24/100], Loss: 9.2727\n",
            "Epoch [25/100], Loss: 9.1937\n",
            "Epoch [26/100], Loss: 9.2321\n",
            "Epoch [27/100], Loss: 8.5320\n",
            "Epoch [28/100], Loss: 8.5492\n",
            "Epoch [29/100], Loss: 8.2986\n",
            "Epoch [30/100], Loss: 8.2666\n",
            "Epoch [31/100], Loss: 8.1737\n",
            "Epoch [32/100], Loss: 8.0082\n",
            "Epoch [33/100], Loss: 7.9365\n",
            "Epoch [34/100], Loss: 7.7767\n",
            "Epoch [35/100], Loss: 7.7562\n",
            "Epoch [36/100], Loss: 7.7990\n",
            "Epoch [37/100], Loss: 7.7412\n",
            "Epoch [38/100], Loss: 7.8437\n",
            "Epoch [39/100], Loss: 7.8033\n",
            "Epoch [40/100], Loss: 7.5538\n",
            "Epoch [41/100], Loss: 7.5671\n",
            "Epoch [42/100], Loss: 7.5799\n",
            "Epoch [43/100], Loss: 7.5482\n",
            "Epoch [44/100], Loss: 7.6474\n",
            "Epoch [45/100], Loss: 7.4652\n",
            "Epoch [46/100], Loss: 7.3484\n",
            "Epoch [47/100], Loss: 7.2709\n",
            "Epoch [48/100], Loss: 7.2726\n",
            "Epoch [49/100], Loss: 7.3464\n",
            "Epoch [50/100], Loss: 7.3357\n",
            "Epoch [51/100], Loss: 7.3577\n",
            "Epoch [52/100], Loss: 7.3567\n",
            "Epoch [53/100], Loss: 7.3767\n",
            "Epoch [54/100], Loss: 7.3216\n",
            "Epoch [55/100], Loss: 7.2157\n",
            "Epoch [56/100], Loss: 7.1445\n",
            "Epoch [57/100], Loss: 7.3280\n",
            "Epoch [58/100], Loss: 7.2254\n",
            "Epoch [59/100], Loss: 7.3759\n",
            "Epoch [60/100], Loss: 7.1983\n",
            "Epoch [61/100], Loss: 7.1794\n",
            "Epoch [62/100], Loss: 7.2954\n",
            "Epoch [63/100], Loss: 7.0892\n",
            "Epoch [64/100], Loss: 7.2032\n",
            "Epoch [65/100], Loss: 6.9769\n",
            "Epoch [66/100], Loss: 7.0811\n",
            "Epoch [67/100], Loss: 7.0211\n",
            "Epoch [68/100], Loss: 7.2541\n",
            "Epoch [69/100], Loss: 7.1678\n",
            "Epoch [70/100], Loss: 7.0784\n",
            "Epoch [71/100], Loss: 6.9618\n",
            "Epoch [72/100], Loss: 6.9869\n",
            "Epoch [73/100], Loss: 7.0538\n",
            "Epoch [74/100], Loss: 6.8928\n",
            "Epoch [75/100], Loss: 6.9469\n",
            "Epoch [76/100], Loss: 6.8765\n",
            "Epoch [77/100], Loss: 6.8900\n",
            "Epoch [78/100], Loss: 6.9104\n",
            "Epoch [79/100], Loss: 6.9986\n",
            "Epoch [80/100], Loss: 6.9595\n",
            "Epoch [81/100], Loss: 6.9185\n",
            "Epoch [82/100], Loss: 6.9916\n",
            "Epoch [83/100], Loss: 6.9373\n",
            "Epoch [84/100], Loss: 6.8026\n",
            "Epoch [85/100], Loss: 6.9299\n",
            "Epoch [86/100], Loss: 6.8169\n",
            "Epoch [87/100], Loss: 6.7947\n",
            "Epoch [88/100], Loss: 6.7696\n",
            "Epoch [89/100], Loss: 6.8511\n",
            "Epoch [90/100], Loss: 6.6811\n",
            "Epoch [91/100], Loss: 6.8970\n",
            "Epoch [92/100], Loss: 6.8692\n",
            "Epoch [93/100], Loss: 6.9075\n",
            "Epoch [94/100], Loss: 6.7750\n",
            "Epoch [95/100], Loss: 6.9624\n",
            "Epoch [96/100], Loss: 6.8124\n",
            "Epoch [97/100], Loss: 6.7778\n",
            "Epoch [98/100], Loss: 6.7679\n",
            "Epoch [99/100], Loss: 6.7638\n",
            "Epoch [100/100], Loss: 6.5239\n",
            "Epoch [1/250], Loss: 581.2094\n",
            "Epoch [2/250], Loss: 503.8508\n",
            "Epoch [3/250], Loss: 387.4343\n",
            "Epoch [4/250], Loss: 248.1360\n",
            "Epoch [5/250], Loss: 122.7566\n",
            "Epoch [6/250], Loss: 54.4788\n",
            "Epoch [7/250], Loss: 44.3322\n",
            "Epoch [8/250], Loss: 36.2751\n",
            "Epoch [9/250], Loss: 27.0261\n",
            "Epoch [10/250], Loss: 21.3587\n",
            "Epoch [11/250], Loss: 18.1190\n",
            "Epoch [12/250], Loss: 15.9876\n",
            "Epoch [13/250], Loss: 14.6070\n",
            "Epoch [14/250], Loss: 13.7054\n",
            "Epoch [15/250], Loss: 12.9502\n",
            "Epoch [16/250], Loss: 12.2783\n",
            "Epoch [17/250], Loss: 11.8550\n",
            "Epoch [18/250], Loss: 11.5657\n",
            "Epoch [19/250], Loss: 10.9737\n",
            "Epoch [20/250], Loss: 10.5546\n",
            "Epoch [21/250], Loss: 10.2444\n",
            "Epoch [22/250], Loss: 10.0760\n",
            "Epoch [23/250], Loss: 9.6474\n",
            "Epoch [24/250], Loss: 9.5342\n",
            "Epoch [25/250], Loss: 9.3067\n",
            "Epoch [26/250], Loss: 9.2126\n",
            "Epoch [27/250], Loss: 9.1714\n",
            "Epoch [28/250], Loss: 8.8928\n",
            "Epoch [29/250], Loss: 8.8280\n",
            "Epoch [30/250], Loss: 8.7118\n",
            "Epoch [31/250], Loss: 8.5599\n",
            "Epoch [32/250], Loss: 8.4262\n",
            "Epoch [33/250], Loss: 8.5443\n",
            "Epoch [34/250], Loss: 8.1676\n",
            "Epoch [35/250], Loss: 8.1290\n",
            "Epoch [36/250], Loss: 7.9976\n",
            "Epoch [37/250], Loss: 8.0011\n",
            "Epoch [38/250], Loss: 7.9019\n",
            "Epoch [39/250], Loss: 7.8605\n",
            "Epoch [40/250], Loss: 7.7663\n",
            "Epoch [41/250], Loss: 7.8106\n",
            "Epoch [42/250], Loss: 7.7426\n",
            "Epoch [43/250], Loss: 7.8642\n",
            "Epoch [44/250], Loss: 7.9406\n",
            "Epoch [45/250], Loss: 7.6846\n",
            "Epoch [46/250], Loss: 7.4261\n",
            "Epoch [47/250], Loss: 7.4675\n",
            "Epoch [48/250], Loss: 7.4081\n",
            "Epoch [49/250], Loss: 7.3550\n",
            "Epoch [50/250], Loss: 7.2167\n",
            "Epoch [51/250], Loss: 7.2083\n",
            "Epoch [52/250], Loss: 7.1379\n",
            "Epoch [53/250], Loss: 7.2877\n",
            "Epoch [54/250], Loss: 7.0911\n",
            "Epoch [55/250], Loss: 7.2713\n",
            "Epoch [56/250], Loss: 7.1323\n",
            "Epoch [57/250], Loss: 7.1509\n",
            "Epoch [58/250], Loss: 7.0967\n",
            "Epoch [59/250], Loss: 7.0343\n",
            "Epoch [60/250], Loss: 7.0263\n",
            "Epoch [61/250], Loss: 7.1328\n",
            "Epoch [62/250], Loss: 7.2808\n",
            "Epoch [63/250], Loss: 7.0081\n",
            "Epoch [64/250], Loss: 7.1741\n",
            "Epoch [65/250], Loss: 6.9884\n",
            "Epoch [66/250], Loss: 6.8582\n",
            "Epoch [67/250], Loss: 7.0902\n",
            "Epoch [68/250], Loss: 6.9604\n",
            "Epoch [69/250], Loss: 6.8552\n",
            "Epoch [70/250], Loss: 6.6973\n",
            "Epoch [71/250], Loss: 6.8504\n",
            "Epoch [72/250], Loss: 6.7550\n",
            "Epoch [73/250], Loss: 6.8346\n",
            "Epoch [74/250], Loss: 6.9218\n",
            "Epoch [75/250], Loss: 6.9911\n",
            "Epoch [76/250], Loss: 7.0119\n",
            "Epoch [77/250], Loss: 6.7808\n",
            "Epoch [78/250], Loss: 6.7825\n",
            "Epoch [79/250], Loss: 6.8000\n",
            "Epoch [80/250], Loss: 6.8780\n",
            "Epoch [81/250], Loss: 6.7106\n",
            "Epoch [82/250], Loss: 6.5643\n",
            "Epoch [83/250], Loss: 6.7270\n",
            "Epoch [84/250], Loss: 6.6938\n",
            "Epoch [85/250], Loss: 6.5033\n",
            "Epoch [86/250], Loss: 6.5889\n",
            "Epoch [87/250], Loss: 6.5142\n",
            "Epoch [88/250], Loss: 6.6228\n",
            "Epoch [89/250], Loss: 6.3905\n",
            "Epoch [90/250], Loss: 6.4743\n",
            "Epoch [91/250], Loss: 6.4220\n",
            "Epoch [92/250], Loss: 6.5187\n",
            "Epoch [93/250], Loss: 6.4353\n",
            "Epoch [94/250], Loss: 6.5188\n",
            "Epoch [95/250], Loss: 6.5474\n",
            "Epoch [96/250], Loss: 6.6040\n",
            "Epoch [97/250], Loss: 6.2947\n",
            "Epoch [98/250], Loss: 6.5406\n",
            "Epoch [99/250], Loss: 6.3066\n",
            "Epoch [100/250], Loss: 6.4894\n",
            "Epoch [101/250], Loss: 6.3835\n",
            "Epoch [102/250], Loss: 6.3515\n",
            "Epoch [103/250], Loss: 6.3120\n",
            "Epoch [104/250], Loss: 6.5033\n",
            "Epoch [105/250], Loss: 6.4256\n",
            "Epoch [106/250], Loss: 6.4296\n",
            "Epoch [107/250], Loss: 6.3696\n",
            "Epoch [108/250], Loss: 6.4136\n",
            "Epoch [109/250], Loss: 6.1833\n",
            "Epoch [110/250], Loss: 6.3177\n",
            "Epoch [111/250], Loss: 6.4241\n",
            "Epoch [112/250], Loss: 6.2404\n",
            "Epoch [113/250], Loss: 6.1936\n",
            "Epoch [114/250], Loss: 6.2410\n",
            "Epoch [115/250], Loss: 6.1612\n",
            "Epoch [116/250], Loss: 6.0952\n",
            "Epoch [117/250], Loss: 6.2444\n",
            "Epoch [118/250], Loss: 6.1198\n",
            "Epoch [119/250], Loss: 6.2482\n",
            "Epoch [120/250], Loss: 6.3108\n",
            "Epoch [121/250], Loss: 6.1095\n",
            "Epoch [122/250], Loss: 6.0269\n",
            "Epoch [123/250], Loss: 6.1708\n",
            "Epoch [124/250], Loss: 6.2343\n",
            "Epoch [125/250], Loss: 6.1341\n",
            "Epoch [126/250], Loss: 6.3769\n",
            "Epoch [127/250], Loss: 6.0884\n",
            "Epoch [128/250], Loss: 5.9762\n",
            "Epoch [129/250], Loss: 6.1418\n",
            "Epoch [130/250], Loss: 6.3409\n",
            "Epoch [131/250], Loss: 6.0479\n",
            "Epoch [132/250], Loss: 5.9598\n",
            "Epoch [133/250], Loss: 5.8804\n",
            "Epoch [134/250], Loss: 6.1627\n",
            "Epoch [135/250], Loss: 5.8939\n",
            "Epoch [136/250], Loss: 5.8928\n",
            "Epoch [137/250], Loss: 5.8669\n",
            "Epoch [138/250], Loss: 5.8299\n",
            "Epoch [139/250], Loss: 5.8326\n",
            "Epoch [140/250], Loss: 6.1110\n",
            "Epoch [141/250], Loss: 6.1140\n",
            "Epoch [142/250], Loss: 6.3043\n",
            "Epoch [143/250], Loss: 6.0124\n",
            "Epoch [144/250], Loss: 5.9559\n",
            "Epoch [145/250], Loss: 6.0433\n",
            "Epoch [146/250], Loss: 5.7779\n",
            "Epoch [147/250], Loss: 5.9292\n",
            "Epoch [148/250], Loss: 6.6251\n",
            "Epoch [149/250], Loss: 6.3568\n",
            "Epoch [150/250], Loss: 5.7596\n",
            "Epoch [151/250], Loss: 5.7500\n",
            "Epoch [152/250], Loss: 5.6597\n",
            "Epoch [153/250], Loss: 5.7137\n",
            "Epoch [154/250], Loss: 5.9742\n",
            "Epoch [155/250], Loss: 5.6817\n",
            "Epoch [156/250], Loss: 5.9325\n",
            "Epoch [157/250], Loss: 5.7849\n",
            "Epoch [158/250], Loss: 5.6580\n",
            "Epoch [159/250], Loss: 5.5583\n",
            "Epoch [160/250], Loss: 5.8525\n",
            "Epoch [161/250], Loss: 5.8501\n",
            "Epoch [162/250], Loss: 6.0378\n",
            "Epoch [163/250], Loss: 5.7070\n",
            "Epoch [164/250], Loss: 5.9150\n",
            "Epoch [165/250], Loss: 5.7776\n",
            "Epoch [166/250], Loss: 5.7997\n",
            "Epoch [167/250], Loss: 5.6904\n",
            "Epoch [168/250], Loss: 5.5898\n",
            "Epoch [169/250], Loss: 5.5926\n",
            "Epoch [170/250], Loss: 5.7223\n",
            "Epoch [171/250], Loss: 5.5823\n",
            "Epoch [172/250], Loss: 5.6306\n",
            "Epoch [173/250], Loss: 5.5647\n",
            "Epoch [174/250], Loss: 5.9575\n",
            "Epoch [175/250], Loss: 5.6979\n",
            "Epoch [176/250], Loss: 5.6772\n",
            "Epoch [177/250], Loss: 5.6840\n",
            "Epoch [178/250], Loss: 5.4538\n",
            "Epoch [179/250], Loss: 5.5957\n",
            "Epoch [180/250], Loss: 5.4422\n",
            "Epoch [181/250], Loss: 5.6599\n",
            "Epoch [182/250], Loss: 5.6904\n",
            "Epoch [183/250], Loss: 5.4976\n",
            "Epoch [184/250], Loss: 5.4114\n",
            "Epoch [185/250], Loss: 5.5108\n",
            "Epoch [186/250], Loss: 5.7926\n",
            "Epoch [187/250], Loss: 5.6021\n",
            "Epoch [188/250], Loss: 5.4932\n",
            "Epoch [189/250], Loss: 5.5757\n",
            "Epoch [190/250], Loss: 5.4280\n",
            "Epoch [191/250], Loss: 5.5351\n",
            "Epoch [192/250], Loss: 5.6832\n",
            "Epoch [193/250], Loss: 5.5393\n",
            "Epoch [194/250], Loss: 5.2848\n",
            "Epoch [195/250], Loss: 5.2946\n",
            "Epoch [196/250], Loss: 5.6743\n",
            "Epoch [197/250], Loss: 5.7459\n",
            "Epoch [198/250], Loss: 6.0094\n",
            "Epoch [199/250], Loss: 5.7827\n",
            "Epoch [200/250], Loss: 5.5313\n",
            "Epoch [201/250], Loss: 5.5021\n",
            "Epoch [202/250], Loss: 5.3460\n",
            "Epoch [203/250], Loss: 5.5211\n",
            "Epoch [204/250], Loss: 5.3613\n",
            "Epoch [205/250], Loss: 5.6442\n",
            "Epoch [206/250], Loss: 5.6707\n",
            "Epoch [207/250], Loss: 5.5409\n",
            "Epoch [208/250], Loss: 5.4167\n",
            "Epoch [209/250], Loss: 5.2753\n",
            "Epoch [210/250], Loss: 5.2796\n",
            "Epoch [211/250], Loss: 5.2642\n",
            "Epoch [212/250], Loss: 5.2115\n",
            "Epoch [213/250], Loss: 5.2714\n",
            "Epoch [214/250], Loss: 5.3381\n",
            "Epoch [215/250], Loss: 5.4593\n",
            "Epoch [216/250], Loss: 5.5645\n",
            "Epoch [217/250], Loss: 5.2882\n",
            "Epoch [218/250], Loss: 5.1302\n",
            "Epoch [219/250], Loss: 5.2584\n",
            "Epoch [220/250], Loss: 5.2767\n",
            "Epoch [221/250], Loss: 5.1660\n",
            "Epoch [222/250], Loss: 5.1630\n",
            "Epoch [223/250], Loss: 5.1439\n",
            "Epoch [224/250], Loss: 5.1931\n",
            "Epoch [225/250], Loss: 5.2642\n",
            "Epoch [226/250], Loss: 5.3996\n",
            "Epoch [227/250], Loss: 5.2927\n",
            "Epoch [228/250], Loss: 5.0856\n",
            "Epoch [229/250], Loss: 5.2635\n",
            "Epoch [230/250], Loss: 5.2321\n",
            "Epoch [231/250], Loss: 5.1429\n",
            "Epoch [232/250], Loss: 5.2074\n",
            "Epoch [233/250], Loss: 5.2750\n",
            "Epoch [234/250], Loss: 5.2645\n",
            "Epoch [235/250], Loss: 5.4409\n",
            "Epoch [236/250], Loss: 5.2735\n",
            "Epoch [237/250], Loss: 5.1949\n",
            "Epoch [238/250], Loss: 5.2377\n",
            "Epoch [239/250], Loss: 5.2431\n",
            "Epoch [240/250], Loss: 5.1135\n",
            "Epoch [241/250], Loss: 5.1141\n",
            "Epoch [242/250], Loss: 5.1344\n",
            "Epoch [243/250], Loss: 5.3470\n",
            "Epoch [244/250], Loss: 5.3039\n",
            "Epoch [245/250], Loss: 5.3268\n",
            "Epoch [246/250], Loss: 5.1763\n",
            "Epoch [247/250], Loss: 5.1056\n",
            "Epoch [248/250], Loss: 5.1964\n",
            "Epoch [249/250], Loss: 5.2562\n",
            "Epoch [250/250], Loss: 5.2559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 4: Comparing Learning Rates\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "results_learning_rates = []\n",
        "for learning_rate in learning_rates:\n",
        "    avg_loss = train_and_evaluate(hidden_layers=[32], activation_fn='relu', learning_rate=learning_rate, batch_size=32, epochs=50)\n",
        "    results_learning_rates.append({'learning_rate': learning_rate, 'avg_loss': avg_loss})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFGwo9HcsUKq",
        "outputId": "a431fbc2-2c56-4c0d-ca13-843cef0abe6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 9270424.2535\n",
            "Epoch [2/50], Loss: 545491.4035\n",
            "Epoch [3/50], Loss: 409122.9387\n",
            "Epoch [4/50], Loss: 75096.6622\n",
            "Epoch [5/50], Loss: 17888.2206\n",
            "Epoch [6/50], Loss: 5291.9846\n",
            "Epoch [7/50], Loss: 7132.9563\n",
            "Epoch [8/50], Loss: 6117.2115\n",
            "Epoch [9/50], Loss: 6229.7807\n",
            "Epoch [10/50], Loss: 1903.3463\n",
            "Epoch [11/50], Loss: 592.8818\n",
            "Epoch [12/50], Loss: 244.4412\n",
            "Epoch [13/50], Loss: 187.6109\n",
            "Epoch [14/50], Loss: 168.1985\n",
            "Epoch [15/50], Loss: 145.2602\n",
            "Epoch [16/50], Loss: 120.4048\n",
            "Epoch [17/50], Loss: 114.3137\n",
            "Epoch [18/50], Loss: 93.9615\n",
            "Epoch [19/50], Loss: 80.6497\n",
            "Epoch [20/50], Loss: 78.2623\n",
            "Epoch [21/50], Loss: 76.8053\n",
            "Epoch [22/50], Loss: 79.3013\n",
            "Epoch [23/50], Loss: 70.8064\n",
            "Epoch [24/50], Loss: 62.3938\n",
            "Epoch [25/50], Loss: 66.0133\n",
            "Epoch [26/50], Loss: 73.9435\n",
            "Epoch [27/50], Loss: 64.1644\n",
            "Epoch [28/50], Loss: 60.7582\n",
            "Epoch [29/50], Loss: 60.3020\n",
            "Epoch [30/50], Loss: 67.3890\n",
            "Epoch [31/50], Loss: 70.4997\n",
            "Epoch [32/50], Loss: 60.3113\n",
            "Epoch [33/50], Loss: 63.8902\n",
            "Epoch [34/50], Loss: 59.4052\n",
            "Epoch [35/50], Loss: 62.4582\n",
            "Epoch [36/50], Loss: 65.7779\n",
            "Epoch [37/50], Loss: 63.2829\n",
            "Epoch [38/50], Loss: 67.6239\n",
            "Epoch [39/50], Loss: 68.2838\n",
            "Epoch [40/50], Loss: 60.9914\n",
            "Epoch [41/50], Loss: 56.1804\n",
            "Epoch [42/50], Loss: 57.5357\n",
            "Epoch [43/50], Loss: 54.9482\n",
            "Epoch [44/50], Loss: 56.4045\n",
            "Epoch [45/50], Loss: 56.1322\n",
            "Epoch [46/50], Loss: 56.4324\n",
            "Epoch [47/50], Loss: 58.7270\n",
            "Epoch [48/50], Loss: 54.7567\n",
            "Epoch [49/50], Loss: 56.3623\n",
            "Epoch [50/50], Loss: 55.6569\n",
            "Epoch [1/50], Loss: 684.6341\n",
            "Epoch [2/50], Loss: 247.3658\n",
            "Epoch [3/50], Loss: 129.1294\n",
            "Epoch [4/50], Loss: 39.3169\n",
            "Epoch [5/50], Loss: 20.1573\n",
            "Epoch [6/50], Loss: 17.2973\n",
            "Epoch [7/50], Loss: 14.9404\n",
            "Epoch [8/50], Loss: 15.5799\n",
            "Epoch [9/50], Loss: 10.6708\n",
            "Epoch [10/50], Loss: 12.2049\n",
            "Epoch [11/50], Loss: 9.7222\n",
            "Epoch [12/50], Loss: 9.4784\n",
            "Epoch [13/50], Loss: 9.1106\n",
            "Epoch [14/50], Loss: 9.3655\n",
            "Epoch [15/50], Loss: 9.5126\n",
            "Epoch [16/50], Loss: 9.1038\n",
            "Epoch [17/50], Loss: 8.9640\n",
            "Epoch [18/50], Loss: 9.6460\n",
            "Epoch [19/50], Loss: 10.6306\n",
            "Epoch [20/50], Loss: 9.1775\n",
            "Epoch [21/50], Loss: 7.7058\n",
            "Epoch [22/50], Loss: 8.4152\n",
            "Epoch [23/50], Loss: 10.3342\n",
            "Epoch [24/50], Loss: 9.1046\n",
            "Epoch [25/50], Loss: 7.9617\n",
            "Epoch [26/50], Loss: 7.3391\n",
            "Epoch [27/50], Loss: 7.1700\n",
            "Epoch [28/50], Loss: 7.8631\n",
            "Epoch [29/50], Loss: 6.8749\n",
            "Epoch [30/50], Loss: 7.5534\n",
            "Epoch [31/50], Loss: 8.2639\n",
            "Epoch [32/50], Loss: 10.0387\n",
            "Epoch [33/50], Loss: 8.7698\n",
            "Epoch [34/50], Loss: 9.4627\n",
            "Epoch [35/50], Loss: 8.6238\n",
            "Epoch [36/50], Loss: 7.6663\n",
            "Epoch [37/50], Loss: 7.7211\n",
            "Epoch [38/50], Loss: 7.3896\n",
            "Epoch [39/50], Loss: 8.8172\n",
            "Epoch [40/50], Loss: 9.3192\n",
            "Epoch [41/50], Loss: 6.7931\n",
            "Epoch [42/50], Loss: 7.9860\n",
            "Epoch [43/50], Loss: 7.5626\n",
            "Epoch [44/50], Loss: 6.9286\n",
            "Epoch [45/50], Loss: 7.1820\n",
            "Epoch [46/50], Loss: 7.8534\n",
            "Epoch [47/50], Loss: 8.5576\n",
            "Epoch [48/50], Loss: 7.8853\n",
            "Epoch [49/50], Loss: 6.9144\n",
            "Epoch [50/50], Loss: 8.2142\n",
            "Epoch [1/50], Loss: 299.0925\n",
            "Epoch [2/50], Loss: 40.5110\n",
            "Epoch [3/50], Loss: 17.6475\n",
            "Epoch [4/50], Loss: 11.9377\n",
            "Epoch [5/50], Loss: 11.7242\n",
            "Epoch [6/50], Loss: 9.4214\n",
            "Epoch [7/50], Loss: 8.4487\n",
            "Epoch [8/50], Loss: 9.2675\n",
            "Epoch [9/50], Loss: 8.9202\n",
            "Epoch [10/50], Loss: 8.3482\n",
            "Epoch [11/50], Loss: 9.3649\n",
            "Epoch [12/50], Loss: 7.9959\n",
            "Epoch [13/50], Loss: 8.3911\n",
            "Epoch [14/50], Loss: 7.5711\n",
            "Epoch [15/50], Loss: 7.7834\n",
            "Epoch [16/50], Loss: 7.1377\n",
            "Epoch [17/50], Loss: 7.5288\n",
            "Epoch [18/50], Loss: 9.3932\n",
            "Epoch [19/50], Loss: 10.0243\n",
            "Epoch [20/50], Loss: 8.9526\n",
            "Epoch [21/50], Loss: 6.7134\n",
            "Epoch [22/50], Loss: 8.3204\n",
            "Epoch [23/50], Loss: 9.8444\n",
            "Epoch [24/50], Loss: 8.9199\n",
            "Epoch [25/50], Loss: 8.4621\n",
            "Epoch [26/50], Loss: 8.3188\n",
            "Epoch [27/50], Loss: 7.7699\n",
            "Epoch [28/50], Loss: 7.3747\n",
            "Epoch [29/50], Loss: 6.8896\n",
            "Epoch [30/50], Loss: 7.2258\n",
            "Epoch [31/50], Loss: 8.2322\n",
            "Epoch [32/50], Loss: 8.4128\n",
            "Epoch [33/50], Loss: 6.4163\n",
            "Epoch [34/50], Loss: 6.9806\n",
            "Epoch [35/50], Loss: 6.7282\n",
            "Epoch [36/50], Loss: 6.7619\n",
            "Epoch [37/50], Loss: 8.3907\n",
            "Epoch [38/50], Loss: 7.1139\n",
            "Epoch [39/50], Loss: 7.3253\n",
            "Epoch [40/50], Loss: 7.8919\n",
            "Epoch [41/50], Loss: 7.9682\n",
            "Epoch [42/50], Loss: 6.7040\n",
            "Epoch [43/50], Loss: 7.6190\n",
            "Epoch [44/50], Loss: 7.9308\n",
            "Epoch [45/50], Loss: 7.5979\n",
            "Epoch [46/50], Loss: 8.2050\n",
            "Epoch [47/50], Loss: 7.3985\n",
            "Epoch [48/50], Loss: 7.7356\n",
            "Epoch [49/50], Loss: 7.7893\n",
            "Epoch [50/50], Loss: 7.7138\n",
            "Epoch [1/50], Loss: 588.4763\n",
            "Epoch [2/50], Loss: 501.0233\n",
            "Epoch [3/50], Loss: 376.9051\n",
            "Epoch [4/50], Loss: 224.9109\n",
            "Epoch [5/50], Loss: 103.6201\n",
            "Epoch [6/50], Loss: 56.5546\n",
            "Epoch [7/50], Loss: 46.9686\n",
            "Epoch [8/50], Loss: 35.2369\n",
            "Epoch [9/50], Loss: 27.1601\n",
            "Epoch [10/50], Loss: 22.5953\n",
            "Epoch [11/50], Loss: 19.6520\n",
            "Epoch [12/50], Loss: 17.6374\n",
            "Epoch [13/50], Loss: 15.6534\n",
            "Epoch [14/50], Loss: 14.3442\n",
            "Epoch [15/50], Loss: 13.2618\n",
            "Epoch [16/50], Loss: 12.6516\n",
            "Epoch [17/50], Loss: 11.9399\n",
            "Epoch [18/50], Loss: 11.3535\n",
            "Epoch [19/50], Loss: 10.9316\n",
            "Epoch [20/50], Loss: 10.4737\n",
            "Epoch [21/50], Loss: 10.2323\n",
            "Epoch [22/50], Loss: 9.9238\n",
            "Epoch [23/50], Loss: 9.8429\n",
            "Epoch [24/50], Loss: 9.5888\n",
            "Epoch [25/50], Loss: 9.5810\n",
            "Epoch [26/50], Loss: 9.3603\n",
            "Epoch [27/50], Loss: 8.8651\n",
            "Epoch [28/50], Loss: 8.8931\n",
            "Epoch [29/50], Loss: 8.4987\n",
            "Epoch [30/50], Loss: 8.3950\n",
            "Epoch [31/50], Loss: 8.2114\n",
            "Epoch [32/50], Loss: 8.2266\n",
            "Epoch [33/50], Loss: 7.9648\n",
            "Epoch [34/50], Loss: 7.9903\n",
            "Epoch [35/50], Loss: 7.6966\n",
            "Epoch [36/50], Loss: 7.6970\n",
            "Epoch [37/50], Loss: 7.4785\n",
            "Epoch [38/50], Loss: 7.5220\n",
            "Epoch [39/50], Loss: 7.4374\n",
            "Epoch [40/50], Loss: 7.4830\n",
            "Epoch [41/50], Loss: 7.5058\n",
            "Epoch [42/50], Loss: 7.3831\n",
            "Epoch [43/50], Loss: 7.3885\n",
            "Epoch [44/50], Loss: 7.1908\n",
            "Epoch [45/50], Loss: 7.2417\n",
            "Epoch [46/50], Loss: 7.1490\n",
            "Epoch [47/50], Loss: 7.1766\n",
            "Epoch [48/50], Loss: 7.0418\n",
            "Epoch [49/50], Loss: 7.0612\n",
            "Epoch [50/50], Loss: 6.9802\n",
            "Epoch [1/50], Loss: 633.5745\n",
            "Epoch [2/50], Loss: 630.9853\n",
            "Epoch [3/50], Loss: 619.8977\n",
            "Epoch [4/50], Loss: 615.6942\n",
            "Epoch [5/50], Loss: 607.1088\n",
            "Epoch [6/50], Loss: 600.0511\n",
            "Epoch [7/50], Loss: 593.8058\n",
            "Epoch [8/50], Loss: 589.4315\n",
            "Epoch [9/50], Loss: 578.9145\n",
            "Epoch [10/50], Loss: 569.6990\n",
            "Epoch [11/50], Loss: 561.3152\n",
            "Epoch [12/50], Loss: 551.6247\n",
            "Epoch [13/50], Loss: 541.2041\n",
            "Epoch [14/50], Loss: 529.3182\n",
            "Epoch [15/50], Loss: 516.9336\n",
            "Epoch [16/50], Loss: 506.8373\n",
            "Epoch [17/50], Loss: 492.5166\n",
            "Epoch [18/50], Loss: 479.5727\n",
            "Epoch [19/50], Loss: 464.7775\n",
            "Epoch [20/50], Loss: 447.6635\n",
            "Epoch [21/50], Loss: 438.6566\n",
            "Epoch [22/50], Loss: 419.8546\n",
            "Epoch [23/50], Loss: 403.4482\n",
            "Epoch [24/50], Loss: 384.4384\n",
            "Epoch [25/50], Loss: 368.8850\n",
            "Epoch [26/50], Loss: 347.6117\n",
            "Epoch [27/50], Loss: 332.9213\n",
            "Epoch [28/50], Loss: 313.0516\n",
            "Epoch [29/50], Loss: 294.6520\n",
            "Epoch [30/50], Loss: 275.9632\n",
            "Epoch [31/50], Loss: 258.6264\n",
            "Epoch [32/50], Loss: 241.5795\n",
            "Epoch [33/50], Loss: 223.5611\n",
            "Epoch [34/50], Loss: 206.7902\n",
            "Epoch [35/50], Loss: 189.2298\n",
            "Epoch [36/50], Loss: 174.7283\n",
            "Epoch [37/50], Loss: 160.0832\n",
            "Epoch [38/50], Loss: 147.2752\n",
            "Epoch [39/50], Loss: 134.4176\n",
            "Epoch [40/50], Loss: 123.2449\n",
            "Epoch [41/50], Loss: 113.1889\n",
            "Epoch [42/50], Loss: 104.6098\n",
            "Epoch [43/50], Loss: 96.2881\n",
            "Epoch [44/50], Loss: 89.3230\n",
            "Epoch [45/50], Loss: 83.3711\n",
            "Epoch [46/50], Loss: 79.0316\n",
            "Epoch [47/50], Loss: 73.6954\n",
            "Epoch [48/50], Loss: 70.5051\n",
            "Epoch [49/50], Loss: 66.2455\n",
            "Epoch [50/50], Loss: 64.0832\n",
            "Epoch [1/50], Loss: 638.2648\n",
            "Epoch [2/50], Loss: 635.5373\n",
            "Epoch [3/50], Loss: 634.8076\n",
            "Epoch [4/50], Loss: 633.2473\n",
            "Epoch [5/50], Loss: 634.8150\n",
            "Epoch [6/50], Loss: 636.4863\n",
            "Epoch [7/50], Loss: 635.1204\n",
            "Epoch [8/50], Loss: 633.9741\n",
            "Epoch [9/50], Loss: 636.5993\n",
            "Epoch [10/50], Loss: 632.8996\n",
            "Epoch [11/50], Loss: 628.8060\n",
            "Epoch [12/50], Loss: 629.3313\n",
            "Epoch [13/50], Loss: 629.7479\n",
            "Epoch [14/50], Loss: 629.7870\n",
            "Epoch [15/50], Loss: 627.6360\n",
            "Epoch [16/50], Loss: 628.3679\n",
            "Epoch [17/50], Loss: 627.9655\n",
            "Epoch [18/50], Loss: 628.4588\n",
            "Epoch [19/50], Loss: 623.9085\n",
            "Epoch [20/50], Loss: 625.3616\n",
            "Epoch [21/50], Loss: 624.8665\n",
            "Epoch [22/50], Loss: 624.4595\n",
            "Epoch [23/50], Loss: 623.5846\n",
            "Epoch [24/50], Loss: 623.7771\n",
            "Epoch [25/50], Loss: 624.4592\n",
            "Epoch [26/50], Loss: 624.3484\n",
            "Epoch [27/50], Loss: 620.8720\n",
            "Epoch [28/50], Loss: 619.9666\n",
            "Epoch [29/50], Loss: 618.6017\n",
            "Epoch [30/50], Loss: 621.6188\n",
            "Epoch [31/50], Loss: 619.3702\n",
            "Epoch [32/50], Loss: 616.0178\n",
            "Epoch [33/50], Loss: 615.9444\n",
            "Epoch [34/50], Loss: 614.9374\n",
            "Epoch [35/50], Loss: 615.5894\n",
            "Epoch [36/50], Loss: 612.8262\n",
            "Epoch [37/50], Loss: 616.9468\n",
            "Epoch [38/50], Loss: 614.5396\n",
            "Epoch [39/50], Loss: 614.0714\n",
            "Epoch [40/50], Loss: 616.7734\n",
            "Epoch [41/50], Loss: 616.0652\n",
            "Epoch [42/50], Loss: 611.7620\n",
            "Epoch [43/50], Loss: 608.8460\n",
            "Epoch [44/50], Loss: 608.6780\n",
            "Epoch [45/50], Loss: 609.1434\n",
            "Epoch [46/50], Loss: 610.2099\n",
            "Epoch [47/50], Loss: 611.0871\n",
            "Epoch [48/50], Loss: 608.7218\n",
            "Epoch [49/50], Loss: 610.2378\n",
            "Epoch [50/50], Loss: 608.3845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 5: Comparing Batch Sizes\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "results_batch_sizes = []\n",
        "for batch_size in batch_sizes:\n",
        "    avg_loss = train_and_evaluate(hidden_layers=[32], activation_fn='relu', learning_rate=0.01, batch_size=batch_size, epochs=50)\n",
        "    results_batch_sizes.append({'batch_size': batch_size, 'avg_loss': avg_loss})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQR9yutosV9g",
        "outputId": "f207410a-5bf4-467c-fa39-36df6409de46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 542.5579\n",
            "Epoch [2/50], Loss: 308.5729\n",
            "Epoch [3/50], Loss: 84.8040\n",
            "Epoch [4/50], Loss: 37.9960\n",
            "Epoch [5/50], Loss: 23.5255\n",
            "Epoch [6/50], Loss: 17.8796\n",
            "Epoch [7/50], Loss: 14.8373\n",
            "Epoch [8/50], Loss: 12.7664\n",
            "Epoch [9/50], Loss: 12.1648\n",
            "Epoch [10/50], Loss: 10.8269\n",
            "Epoch [11/50], Loss: 10.6051\n",
            "Epoch [12/50], Loss: 9.6594\n",
            "Epoch [13/50], Loss: 9.2350\n",
            "Epoch [14/50], Loss: 9.1053\n",
            "Epoch [15/50], Loss: 8.4102\n",
            "Epoch [16/50], Loss: 8.4390\n",
            "Epoch [17/50], Loss: 8.0836\n",
            "Epoch [18/50], Loss: 8.1038\n",
            "Epoch [19/50], Loss: 8.0018\n",
            "Epoch [20/50], Loss: 7.8582\n",
            "Epoch [21/50], Loss: 7.6623\n",
            "Epoch [22/50], Loss: 7.6817\n",
            "Epoch [23/50], Loss: 7.6100\n",
            "Epoch [24/50], Loss: 7.3919\n",
            "Epoch [25/50], Loss: 7.3245\n",
            "Epoch [26/50], Loss: 7.2832\n",
            "Epoch [27/50], Loss: 7.3846\n",
            "Epoch [28/50], Loss: 7.1158\n",
            "Epoch [29/50], Loss: 7.0228\n",
            "Epoch [30/50], Loss: 7.3979\n",
            "Epoch [31/50], Loss: 7.5129\n",
            "Epoch [32/50], Loss: 7.1068\n",
            "Epoch [33/50], Loss: 7.1688\n",
            "Epoch [34/50], Loss: 6.9342\n",
            "Epoch [35/50], Loss: 6.7546\n",
            "Epoch [36/50], Loss: 7.2120\n",
            "Epoch [37/50], Loss: 7.3602\n",
            "Epoch [38/50], Loss: 6.7997\n",
            "Epoch [39/50], Loss: 6.7255\n",
            "Epoch [40/50], Loss: 6.6872\n",
            "Epoch [41/50], Loss: 6.7211\n",
            "Epoch [42/50], Loss: 6.7723\n",
            "Epoch [43/50], Loss: 6.7006\n",
            "Epoch [44/50], Loss: 6.7355\n",
            "Epoch [45/50], Loss: 6.5825\n",
            "Epoch [46/50], Loss: 6.5259\n",
            "Epoch [47/50], Loss: 6.8940\n",
            "Epoch [48/50], Loss: 6.6867\n",
            "Epoch [49/50], Loss: 6.7148\n",
            "Epoch [50/50], Loss: 6.6998\n",
            "Epoch [1/50], Loss: 584.8381\n",
            "Epoch [2/50], Loss: 488.7291\n",
            "Epoch [3/50], Loss: 351.5754\n",
            "Epoch [4/50], Loss: 185.4226\n",
            "Epoch [5/50], Loss: 77.2108\n",
            "Epoch [6/50], Loss: 52.0672\n",
            "Epoch [7/50], Loss: 40.9449\n",
            "Epoch [8/50], Loss: 28.9216\n",
            "Epoch [9/50], Loss: 23.4660\n",
            "Epoch [10/50], Loss: 19.7345\n",
            "Epoch [11/50], Loss: 17.0864\n",
            "Epoch [12/50], Loss: 15.4159\n",
            "Epoch [13/50], Loss: 13.8009\n",
            "Epoch [14/50], Loss: 12.9520\n",
            "Epoch [15/50], Loss: 12.0601\n",
            "Epoch [16/50], Loss: 11.4781\n",
            "Epoch [17/50], Loss: 10.8511\n",
            "Epoch [18/50], Loss: 10.7163\n",
            "Epoch [19/50], Loss: 10.0055\n",
            "Epoch [20/50], Loss: 9.8716\n",
            "Epoch [21/50], Loss: 9.4399\n",
            "Epoch [22/50], Loss: 9.4462\n",
            "Epoch [23/50], Loss: 9.3149\n",
            "Epoch [24/50], Loss: 9.0817\n",
            "Epoch [25/50], Loss: 9.1349\n",
            "Epoch [26/50], Loss: 8.7423\n",
            "Epoch [27/50], Loss: 8.6226\n",
            "Epoch [28/50], Loss: 8.4486\n",
            "Epoch [29/50], Loss: 8.5659\n",
            "Epoch [30/50], Loss: 8.3349\n",
            "Epoch [31/50], Loss: 8.2413\n",
            "Epoch [32/50], Loss: 8.1702\n",
            "Epoch [33/50], Loss: 8.0409\n",
            "Epoch [34/50], Loss: 8.1279\n",
            "Epoch [35/50], Loss: 7.7412\n",
            "Epoch [36/50], Loss: 7.7101\n",
            "Epoch [37/50], Loss: 7.9036\n",
            "Epoch [38/50], Loss: 7.6810\n",
            "Epoch [39/50], Loss: 7.8164\n",
            "Epoch [40/50], Loss: 7.7541\n",
            "Epoch [41/50], Loss: 7.5769\n",
            "Epoch [42/50], Loss: 7.5037\n",
            "Epoch [43/50], Loss: 7.6197\n",
            "Epoch [44/50], Loss: 7.3553\n",
            "Epoch [45/50], Loss: 7.3561\n",
            "Epoch [46/50], Loss: 7.3573\n",
            "Epoch [47/50], Loss: 7.2967\n",
            "Epoch [48/50], Loss: 7.2412\n",
            "Epoch [49/50], Loss: 7.2427\n",
            "Epoch [50/50], Loss: 7.2323\n",
            "Epoch [1/50], Loss: 592.6196\n",
            "Epoch [2/50], Loss: 555.4003\n",
            "Epoch [3/50], Loss: 509.0601\n",
            "Epoch [4/50], Loss: 451.6016\n",
            "Epoch [5/50], Loss: 383.0032\n",
            "Epoch [6/50], Loss: 303.7063\n",
            "Epoch [7/50], Loss: 222.0507\n",
            "Epoch [8/50], Loss: 147.9669\n",
            "Epoch [9/50], Loss: 90.5342\n",
            "Epoch [10/50], Loss: 63.4608\n",
            "Epoch [11/50], Loss: 57.2444\n",
            "Epoch [12/50], Loss: 55.4692\n",
            "Epoch [13/50], Loss: 47.8076\n",
            "Epoch [14/50], Loss: 39.3392\n",
            "Epoch [15/50], Loss: 32.8642\n",
            "Epoch [16/50], Loss: 29.1415\n",
            "Epoch [17/50], Loss: 26.2448\n",
            "Epoch [18/50], Loss: 23.9278\n",
            "Epoch [19/50], Loss: 21.8664\n",
            "Epoch [20/50], Loss: 19.9774\n",
            "Epoch [21/50], Loss: 18.6499\n",
            "Epoch [22/50], Loss: 17.5286\n",
            "Epoch [23/50], Loss: 16.4878\n",
            "Epoch [24/50], Loss: 15.8318\n",
            "Epoch [25/50], Loss: 15.0645\n",
            "Epoch [26/50], Loss: 14.5725\n",
            "Epoch [27/50], Loss: 14.2184\n",
            "Epoch [28/50], Loss: 13.7917\n",
            "Epoch [29/50], Loss: 13.4702\n",
            "Epoch [30/50], Loss: 13.1163\n",
            "Epoch [31/50], Loss: 12.7919\n",
            "Epoch [32/50], Loss: 12.5994\n",
            "Epoch [33/50], Loss: 12.2901\n",
            "Epoch [34/50], Loss: 11.9516\n",
            "Epoch [35/50], Loss: 11.7554\n",
            "Epoch [36/50], Loss: 11.5952\n",
            "Epoch [37/50], Loss: 11.4224\n",
            "Epoch [38/50], Loss: 11.1268\n",
            "Epoch [39/50], Loss: 10.9677\n",
            "Epoch [40/50], Loss: 10.7792\n",
            "Epoch [41/50], Loss: 10.5035\n",
            "Epoch [42/50], Loss: 10.4171\n",
            "Epoch [43/50], Loss: 10.3623\n",
            "Epoch [44/50], Loss: 10.1533\n",
            "Epoch [45/50], Loss: 9.9638\n",
            "Epoch [46/50], Loss: 9.8432\n",
            "Epoch [47/50], Loss: 9.7835\n",
            "Epoch [48/50], Loss: 9.4537\n",
            "Epoch [49/50], Loss: 9.4514\n",
            "Epoch [50/50], Loss: 9.2381\n",
            "Epoch [1/50], Loss: 623.0364\n",
            "Epoch [2/50], Loss: 595.4762\n",
            "Epoch [3/50], Loss: 581.4108\n",
            "Epoch [4/50], Loss: 550.5670\n",
            "Epoch [5/50], Loss: 519.7106\n",
            "Epoch [6/50], Loss: 496.0920\n",
            "Epoch [7/50], Loss: 470.2716\n",
            "Epoch [8/50], Loss: 426.4470\n",
            "Epoch [9/50], Loss: 368.0546\n",
            "Epoch [10/50], Loss: 337.0622\n",
            "Epoch [11/50], Loss: 294.2324\n",
            "Epoch [12/50], Loss: 244.2520\n",
            "Epoch [13/50], Loss: 193.9670\n",
            "Epoch [14/50], Loss: 155.1329\n",
            "Epoch [15/50], Loss: 122.2796\n",
            "Epoch [16/50], Loss: 88.5536\n",
            "Epoch [17/50], Loss: 75.3155\n",
            "Epoch [18/50], Loss: 64.1910\n",
            "Epoch [19/50], Loss: 59.2854\n",
            "Epoch [20/50], Loss: 56.4044\n",
            "Epoch [21/50], Loss: 58.2106\n",
            "Epoch [22/50], Loss: 55.0735\n",
            "Epoch [23/50], Loss: 49.9966\n",
            "Epoch [24/50], Loss: 46.3357\n",
            "Epoch [25/50], Loss: 43.5816\n",
            "Epoch [26/50], Loss: 36.0153\n",
            "Epoch [27/50], Loss: 34.7550\n",
            "Epoch [28/50], Loss: 34.0175\n",
            "Epoch [29/50], Loss: 31.5007\n",
            "Epoch [30/50], Loss: 30.0706\n",
            "Epoch [31/50], Loss: 28.0170\n",
            "Epoch [32/50], Loss: 25.4235\n",
            "Epoch [33/50], Loss: 25.8930\n",
            "Epoch [34/50], Loss: 22.6671\n",
            "Epoch [35/50], Loss: 23.2000\n",
            "Epoch [36/50], Loss: 21.6196\n",
            "Epoch [37/50], Loss: 20.2052\n",
            "Epoch [38/50], Loss: 17.4802\n",
            "Epoch [39/50], Loss: 18.3430\n",
            "Epoch [40/50], Loss: 16.5459\n",
            "Epoch [41/50], Loss: 17.8921\n",
            "Epoch [42/50], Loss: 16.2174\n",
            "Epoch [43/50], Loss: 14.7108\n",
            "Epoch [44/50], Loss: 14.3398\n",
            "Epoch [45/50], Loss: 14.0890\n",
            "Epoch [46/50], Loss: 13.6523\n",
            "Epoch [47/50], Loss: 13.8650\n",
            "Epoch [48/50], Loss: 13.6975\n",
            "Epoch [49/50], Loss: 12.7352\n",
            "Epoch [50/50], Loss: 13.8175\n",
            "Epoch [1/50], Loss: 631.4048\n",
            "Epoch [2/50], Loss: 613.3022\n",
            "Epoch [3/50], Loss: 623.7587\n",
            "Epoch [4/50], Loss: 599.2991\n",
            "Epoch [5/50], Loss: 572.5354\n",
            "Epoch [6/50], Loss: 556.7227\n",
            "Epoch [7/50], Loss: 528.1779\n",
            "Epoch [8/50], Loss: 511.8123\n",
            "Epoch [9/50], Loss: 514.3473\n",
            "Epoch [10/50], Loss: 481.7777\n",
            "Epoch [11/50], Loss: 430.9516\n",
            "Epoch [12/50], Loss: 409.9360\n",
            "Epoch [13/50], Loss: 364.7671\n",
            "Epoch [14/50], Loss: 359.3748\n",
            "Epoch [15/50], Loss: 320.3520\n",
            "Epoch [16/50], Loss: 268.1774\n",
            "Epoch [17/50], Loss: 232.4929\n",
            "Epoch [18/50], Loss: 212.7145\n",
            "Epoch [19/50], Loss: 172.1588\n",
            "Epoch [20/50], Loss: 163.5905\n",
            "Epoch [21/50], Loss: 129.4817\n",
            "Epoch [22/50], Loss: 109.6649\n",
            "Epoch [23/50], Loss: 102.0556\n",
            "Epoch [24/50], Loss: 91.6583\n",
            "Epoch [25/50], Loss: 75.0967\n",
            "Epoch [26/50], Loss: 80.4776\n",
            "Epoch [27/50], Loss: 69.9265\n",
            "Epoch [28/50], Loss: 73.8326\n",
            "Epoch [29/50], Loss: 76.1733\n",
            "Epoch [30/50], Loss: 71.7627\n",
            "Epoch [31/50], Loss: 70.6023\n",
            "Epoch [32/50], Loss: 59.5791\n",
            "Epoch [33/50], Loss: 55.8517\n",
            "Epoch [34/50], Loss: 58.4457\n",
            "Epoch [35/50], Loss: 55.3058\n",
            "Epoch [36/50], Loss: 43.0543\n",
            "Epoch [37/50], Loss: 40.5695\n",
            "Epoch [38/50], Loss: 50.2956\n",
            "Epoch [39/50], Loss: 37.4615\n",
            "Epoch [40/50], Loss: 42.9930\n",
            "Epoch [41/50], Loss: 34.4103\n",
            "Epoch [42/50], Loss: 38.6791\n",
            "Epoch [43/50], Loss: 33.9847\n",
            "Epoch [44/50], Loss: 31.2973\n",
            "Epoch [45/50], Loss: 32.7230\n",
            "Epoch [46/50], Loss: 32.2665\n",
            "Epoch [47/50], Loss: 27.3272\n",
            "Epoch [48/50], Loss: 26.8983\n",
            "Epoch [49/50], Loss: 25.7839\n",
            "Epoch [50/50], Loss: 23.1489\n",
            "Epoch [1/50], Loss: 607.1202\n",
            "Epoch [2/50], Loss: 599.0472\n",
            "Epoch [3/50], Loss: 590.8398\n",
            "Epoch [4/50], Loss: 582.4453\n",
            "Epoch [5/50], Loss: 573.8094\n",
            "Epoch [6/50], Loss: 564.8869\n",
            "Epoch [7/50], Loss: 555.6323\n",
            "Epoch [8/50], Loss: 546.0117\n",
            "Epoch [9/50], Loss: 536.0018\n",
            "Epoch [10/50], Loss: 525.5752\n",
            "Epoch [11/50], Loss: 514.7253\n",
            "Epoch [12/50], Loss: 503.4313\n",
            "Epoch [13/50], Loss: 491.6622\n",
            "Epoch [14/50], Loss: 479.4031\n",
            "Epoch [15/50], Loss: 466.6563\n",
            "Epoch [16/50], Loss: 453.4309\n",
            "Epoch [17/50], Loss: 439.7362\n",
            "Epoch [18/50], Loss: 425.5840\n",
            "Epoch [19/50], Loss: 410.9872\n",
            "Epoch [20/50], Loss: 395.9829\n",
            "Epoch [21/50], Loss: 380.6102\n",
            "Epoch [22/50], Loss: 364.9012\n",
            "Epoch [23/50], Loss: 348.9079\n",
            "Epoch [24/50], Loss: 332.6753\n",
            "Epoch [25/50], Loss: 316.2659\n",
            "Epoch [26/50], Loss: 299.7416\n",
            "Epoch [27/50], Loss: 283.1697\n",
            "Epoch [28/50], Loss: 266.6118\n",
            "Epoch [29/50], Loss: 250.1654\n",
            "Epoch [30/50], Loss: 233.9233\n",
            "Epoch [31/50], Loss: 217.9770\n",
            "Epoch [32/50], Loss: 202.4253\n",
            "Epoch [33/50], Loss: 187.3667\n",
            "Epoch [34/50], Loss: 172.9020\n",
            "Epoch [35/50], Loss: 159.1244\n",
            "Epoch [36/50], Loss: 146.1281\n",
            "Epoch [37/50], Loss: 134.0022\n",
            "Epoch [38/50], Loss: 122.8236\n",
            "Epoch [39/50], Loss: 112.6547\n",
            "Epoch [40/50], Loss: 103.5516\n",
            "Epoch [41/50], Loss: 95.5398\n",
            "Epoch [42/50], Loss: 88.6275\n",
            "Epoch [43/50], Loss: 82.7949\n",
            "Epoch [44/50], Loss: 77.9960\n",
            "Epoch [45/50], Loss: 74.1603\n",
            "Epoch [46/50], Loss: 71.1921\n",
            "Epoch [47/50], Loss: 68.9774\n",
            "Epoch [48/50], Loss: 67.3858\n",
            "Epoch [49/50], Loss: 66.2755\n",
            "Epoch [50/50], Loss: 65.5068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate and save results\n",
        "hidden_layers_df = pd.DataFrame(results_hidden_layers)\n",
        "activation_functions_df = pd.DataFrame(results_activation_functions)\n",
        "epochs_df = pd.DataFrame(results_epochs)\n",
        "learning_rates_df = pd.DataFrame(results_learning_rates)\n",
        "batch_sizes_df = pd.DataFrame(results_batch_sizes)\n",
        "\n",
        "# Save each result separately\n",
        "hidden_layers_df.to_csv('mlp_hidden_layers_results_regression.csv', index=False)\n",
        "activation_functions_df.to_csv('mlp_activation_functions_results_regression.csv', index=False)\n",
        "epochs_df.to_csv('mlp_epochs_results_regression.csv', index=False)\n",
        "learning_rates_df.to_csv('mlp_learning_rates_results_regression.csv', index=False)\n",
        "batch_sizes_df.to_csv('mlp_batch_sizes_results_regression.csv', index=False)\n",
        "\n",
        "print(\"Results saved for each experiment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC_vqlbmsX3j",
        "outputId": "9718eabf-700a-4a11-a423-2aeca163a228"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved for each experiment.\n"
          ]
        }
      ]
    }
  ]
}