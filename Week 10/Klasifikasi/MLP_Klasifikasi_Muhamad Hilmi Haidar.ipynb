{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QZj_70mgnjxU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('/content/car.data', header=None)\n",
        "data.columns = ['Buying', 'Maint', 'Doors', 'Persons', 'Lug_Boot', 'Safety', 'Class']  # Assign column names"
      ],
      "metadata": {
        "id": "1TQsMLlknuQD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features using LabelEncoder\n",
        "label_encoders = {}\n",
        "for col in data.columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    data[col] = label_encoders[col].fit_transform(data[col])  # Encode labels numerically\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data.drop('Class', axis=1).values  # Features\n",
        "y = data['Class'].values  # Target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)  # Normalize features to have mean 0 and variance 1"
      ],
      "metadata": {
        "id": "zRf4vm1nn4PA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "t6kqwrMgn6Jj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom Dataset class for PyTorch\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "0sPyhBsen8SU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model class\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
        "        super(MLPModel, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        for i, hidden_size in enumerate(hidden_layers):\n",
        "            layers.append(nn.Linear(input_size if i == 0 else hidden_layers[i-1], hidden_size))\n",
        "            if activation_fn == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation_fn == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation_fn == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif activation_fn == 'softmax':\n",
        "                layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_layers[-1], len(set(y))))  # Output layer\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Ow4C2k93n90z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to train and evaluate the model\n",
        "def train_and_evaluate(hidden_layers, activation_fn, learning_rate, batch_size, epochs):\n",
        "    # Initialize dataset and dataloaders\n",
        "    train_dataset = CustomDataset(X_train, y_train)\n",
        "    test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MLPModel(input_size=X_train.shape[1], hidden_layers=hidden_layers, activation_fn=activation_fn).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")  # Verbose output\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "-Z3TYlDTn_tV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Comparing Hidden Layers and Neurons\n",
        "hidden_layers_options = [[4], [8], [16], [32], [64], [128]]\n",
        "results_hidden_layers = []\n",
        "for hidden_layers in hidden_layers_options:\n",
        "    accuracy = train_and_evaluate(hidden_layers, activation_fn='relu', learning_rate=0.01, batch_size=32, epochs=50)\n",
        "    results_hidden_layers.append({'hidden_layers': hidden_layers, 'accuracy': accuracy})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycez9RPfoCQv",
        "outputId": "08892bdb-8bac-45dd-d382-1bce4c4d61b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.0203\n",
            "Epoch [2/50], Loss: 0.7019\n",
            "Epoch [3/50], Loss: 0.6646\n",
            "Epoch [4/50], Loss: 0.6241\n",
            "Epoch [5/50], Loss: 0.6135\n",
            "Epoch [6/50], Loss: 0.5853\n",
            "Epoch [7/50], Loss: 0.5894\n",
            "Epoch [8/50], Loss: 0.5773\n",
            "Epoch [9/50], Loss: 0.5559\n",
            "Epoch [10/50], Loss: 0.5568\n",
            "Epoch [11/50], Loss: 0.5450\n",
            "Epoch [12/50], Loss: 0.5167\n",
            "Epoch [13/50], Loss: 0.5007\n",
            "Epoch [14/50], Loss: 0.5019\n",
            "Epoch [15/50], Loss: 0.5089\n",
            "Epoch [16/50], Loss: 0.4880\n",
            "Epoch [17/50], Loss: 0.4832\n",
            "Epoch [18/50], Loss: 0.4872\n",
            "Epoch [19/50], Loss: 0.4866\n",
            "Epoch [20/50], Loss: 0.4706\n",
            "Epoch [21/50], Loss: 0.4673\n",
            "Epoch [22/50], Loss: 0.4708\n",
            "Epoch [23/50], Loss: 0.4547\n",
            "Epoch [24/50], Loss: 0.4605\n",
            "Epoch [25/50], Loss: 0.4407\n",
            "Epoch [26/50], Loss: 0.4417\n",
            "Epoch [27/50], Loss: 0.4373\n",
            "Epoch [28/50], Loss: 0.4288\n",
            "Epoch [29/50], Loss: 0.4242\n",
            "Epoch [30/50], Loss: 0.4223\n",
            "Epoch [31/50], Loss: 0.4158\n",
            "Epoch [32/50], Loss: 0.4117\n",
            "Epoch [33/50], Loss: 0.4125\n",
            "Epoch [34/50], Loss: 0.4127\n",
            "Epoch [35/50], Loss: 0.4105\n",
            "Epoch [36/50], Loss: 0.4124\n",
            "Epoch [37/50], Loss: 0.4059\n",
            "Epoch [38/50], Loss: 0.4068\n",
            "Epoch [39/50], Loss: 0.4070\n",
            "Epoch [40/50], Loss: 0.3998\n",
            "Epoch [41/50], Loss: 0.4190\n",
            "Epoch [42/50], Loss: 0.3996\n",
            "Epoch [43/50], Loss: 0.4033\n",
            "Epoch [44/50], Loss: 0.4027\n",
            "Epoch [45/50], Loss: 0.4006\n",
            "Epoch [46/50], Loss: 0.4110\n",
            "Epoch [47/50], Loss: 0.4060\n",
            "Epoch [48/50], Loss: 0.3991\n",
            "Epoch [49/50], Loss: 0.3988\n",
            "Epoch [50/50], Loss: 0.4006\n",
            "Epoch [1/50], Loss: 0.9755\n",
            "Epoch [2/50], Loss: 0.6904\n",
            "Epoch [3/50], Loss: 0.6687\n",
            "Epoch [4/50], Loss: 0.6528\n",
            "Epoch [5/50], Loss: 0.6140\n",
            "Epoch [6/50], Loss: 0.6029\n",
            "Epoch [7/50], Loss: 0.5673\n",
            "Epoch [8/50], Loss: 0.5419\n",
            "Epoch [9/50], Loss: 0.5215\n",
            "Epoch [10/50], Loss: 0.4974\n",
            "Epoch [11/50], Loss: 0.4706\n",
            "Epoch [12/50], Loss: 0.4462\n",
            "Epoch [13/50], Loss: 0.4168\n",
            "Epoch [14/50], Loss: 0.3932\n",
            "Epoch [15/50], Loss: 0.3903\n",
            "Epoch [16/50], Loss: 0.3666\n",
            "Epoch [17/50], Loss: 0.3546\n",
            "Epoch [18/50], Loss: 0.3366\n",
            "Epoch [19/50], Loss: 0.3305\n",
            "Epoch [20/50], Loss: 0.3161\n",
            "Epoch [21/50], Loss: 0.3063\n",
            "Epoch [22/50], Loss: 0.2877\n",
            "Epoch [23/50], Loss: 0.2802\n",
            "Epoch [24/50], Loss: 0.2655\n",
            "Epoch [25/50], Loss: 0.2487\n",
            "Epoch [26/50], Loss: 0.2414\n",
            "Epoch [27/50], Loss: 0.2326\n",
            "Epoch [28/50], Loss: 0.2358\n",
            "Epoch [29/50], Loss: 0.2263\n",
            "Epoch [30/50], Loss: 0.2110\n",
            "Epoch [31/50], Loss: 0.2066\n",
            "Epoch [32/50], Loss: 0.2062\n",
            "Epoch [33/50], Loss: 0.2010\n",
            "Epoch [34/50], Loss: 0.1935\n",
            "Epoch [35/50], Loss: 0.2026\n",
            "Epoch [36/50], Loss: 0.2069\n",
            "Epoch [37/50], Loss: 0.1825\n",
            "Epoch [38/50], Loss: 0.1837\n",
            "Epoch [39/50], Loss: 0.1734\n",
            "Epoch [40/50], Loss: 0.1734\n",
            "Epoch [41/50], Loss: 0.1726\n",
            "Epoch [42/50], Loss: 0.1619\n",
            "Epoch [43/50], Loss: 0.1593\n",
            "Epoch [44/50], Loss: 0.1567\n",
            "Epoch [45/50], Loss: 0.1537\n",
            "Epoch [46/50], Loss: 0.1545\n",
            "Epoch [47/50], Loss: 0.1554\n",
            "Epoch [48/50], Loss: 0.1475\n",
            "Epoch [49/50], Loss: 0.1568\n",
            "Epoch [50/50], Loss: 0.1496\n",
            "Epoch [1/50], Loss: 0.9064\n",
            "Epoch [2/50], Loss: 0.6444\n",
            "Epoch [3/50], Loss: 0.5708\n",
            "Epoch [4/50], Loss: 0.4736\n",
            "Epoch [5/50], Loss: 0.3789\n",
            "Epoch [6/50], Loss: 0.3164\n",
            "Epoch [7/50], Loss: 0.2823\n",
            "Epoch [8/50], Loss: 0.2519\n",
            "Epoch [9/50], Loss: 0.2252\n",
            "Epoch [10/50], Loss: 0.2044\n",
            "Epoch [11/50], Loss: 0.1904\n",
            "Epoch [12/50], Loss: 0.1723\n",
            "Epoch [13/50], Loss: 0.1663\n",
            "Epoch [14/50], Loss: 0.1614\n",
            "Epoch [15/50], Loss: 0.1512\n",
            "Epoch [16/50], Loss: 0.1402\n",
            "Epoch [17/50], Loss: 0.1353\n",
            "Epoch [18/50], Loss: 0.1282\n",
            "Epoch [19/50], Loss: 0.1194\n",
            "Epoch [20/50], Loss: 0.1190\n",
            "Epoch [21/50], Loss: 0.1125\n",
            "Epoch [22/50], Loss: 0.1117\n",
            "Epoch [23/50], Loss: 0.1086\n",
            "Epoch [24/50], Loss: 0.1117\n",
            "Epoch [25/50], Loss: 0.0998\n",
            "Epoch [26/50], Loss: 0.0921\n",
            "Epoch [27/50], Loss: 0.0924\n",
            "Epoch [28/50], Loss: 0.0893\n",
            "Epoch [29/50], Loss: 0.0865\n",
            "Epoch [30/50], Loss: 0.0887\n",
            "Epoch [31/50], Loss: 0.0843\n",
            "Epoch [32/50], Loss: 0.0845\n",
            "Epoch [33/50], Loss: 0.1023\n",
            "Epoch [34/50], Loss: 0.0955\n",
            "Epoch [35/50], Loss: 0.0761\n",
            "Epoch [36/50], Loss: 0.0736\n",
            "Epoch [37/50], Loss: 0.0722\n",
            "Epoch [38/50], Loss: 0.0687\n",
            "Epoch [39/50], Loss: 0.0692\n",
            "Epoch [40/50], Loss: 0.0716\n",
            "Epoch [41/50], Loss: 0.0694\n",
            "Epoch [42/50], Loss: 0.0680\n",
            "Epoch [43/50], Loss: 0.0752\n",
            "Epoch [44/50], Loss: 0.0766\n",
            "Epoch [45/50], Loss: 0.0638\n",
            "Epoch [46/50], Loss: 0.0624\n",
            "Epoch [47/50], Loss: 0.0608\n",
            "Epoch [48/50], Loss: 0.0576\n",
            "Epoch [49/50], Loss: 0.0591\n",
            "Epoch [50/50], Loss: 0.0782\n",
            "Epoch [1/50], Loss: 0.7527\n",
            "Epoch [2/50], Loss: 0.5792\n",
            "Epoch [3/50], Loss: 0.4280\n",
            "Epoch [4/50], Loss: 0.3353\n",
            "Epoch [5/50], Loss: 0.2768\n",
            "Epoch [6/50], Loss: 0.2289\n",
            "Epoch [7/50], Loss: 0.2060\n",
            "Epoch [8/50], Loss: 0.1812\n",
            "Epoch [9/50], Loss: 0.1577\n",
            "Epoch [10/50], Loss: 0.1405\n",
            "Epoch [11/50], Loss: 0.1197\n",
            "Epoch [12/50], Loss: 0.1149\n",
            "Epoch [13/50], Loss: 0.1050\n",
            "Epoch [14/50], Loss: 0.0984\n",
            "Epoch [15/50], Loss: 0.0866\n",
            "Epoch [16/50], Loss: 0.0832\n",
            "Epoch [17/50], Loss: 0.0776\n",
            "Epoch [18/50], Loss: 0.0759\n",
            "Epoch [19/50], Loss: 0.0734\n",
            "Epoch [20/50], Loss: 0.0739\n",
            "Epoch [21/50], Loss: 0.0685\n",
            "Epoch [22/50], Loss: 0.0630\n",
            "Epoch [23/50], Loss: 0.0596\n",
            "Epoch [24/50], Loss: 0.0661\n",
            "Epoch [25/50], Loss: 0.0529\n",
            "Epoch [26/50], Loss: 0.0504\n",
            "Epoch [27/50], Loss: 0.0478\n",
            "Epoch [28/50], Loss: 0.0454\n",
            "Epoch [29/50], Loss: 0.0398\n",
            "Epoch [30/50], Loss: 0.0471\n",
            "Epoch [31/50], Loss: 0.0419\n",
            "Epoch [32/50], Loss: 0.0422\n",
            "Epoch [33/50], Loss: 0.0438\n",
            "Epoch [34/50], Loss: 0.0372\n",
            "Epoch [35/50], Loss: 0.0361\n",
            "Epoch [36/50], Loss: 0.0355\n",
            "Epoch [37/50], Loss: 0.0329\n",
            "Epoch [38/50], Loss: 0.0355\n",
            "Epoch [39/50], Loss: 0.0306\n",
            "Epoch [40/50], Loss: 0.0281\n",
            "Epoch [41/50], Loss: 0.0282\n",
            "Epoch [42/50], Loss: 0.0338\n",
            "Epoch [43/50], Loss: 0.0299\n",
            "Epoch [44/50], Loss: 0.0255\n",
            "Epoch [45/50], Loss: 0.0284\n",
            "Epoch [46/50], Loss: 0.0282\n",
            "Epoch [47/50], Loss: 0.0245\n",
            "Epoch [48/50], Loss: 0.0275\n",
            "Epoch [49/50], Loss: 0.0239\n",
            "Epoch [50/50], Loss: 0.0234\n",
            "Epoch [1/50], Loss: 0.7525\n",
            "Epoch [2/50], Loss: 0.5427\n",
            "Epoch [3/50], Loss: 0.3832\n",
            "Epoch [4/50], Loss: 0.2830\n",
            "Epoch [5/50], Loss: 0.2228\n",
            "Epoch [6/50], Loss: 0.1729\n",
            "Epoch [7/50], Loss: 0.1534\n",
            "Epoch [8/50], Loss: 0.1309\n",
            "Epoch [9/50], Loss: 0.1300\n",
            "Epoch [10/50], Loss: 0.1119\n",
            "Epoch [11/50], Loss: 0.1046\n",
            "Epoch [12/50], Loss: 0.1001\n",
            "Epoch [13/50], Loss: 0.0876\n",
            "Epoch [14/50], Loss: 0.0808\n",
            "Epoch [15/50], Loss: 0.0709\n",
            "Epoch [16/50], Loss: 0.0704\n",
            "Epoch [17/50], Loss: 0.0666\n",
            "Epoch [18/50], Loss: 0.0538\n",
            "Epoch [19/50], Loss: 0.0606\n",
            "Epoch [20/50], Loss: 0.0544\n",
            "Epoch [21/50], Loss: 0.0540\n",
            "Epoch [22/50], Loss: 0.0499\n",
            "Epoch [23/50], Loss: 0.0403\n",
            "Epoch [24/50], Loss: 0.0380\n",
            "Epoch [25/50], Loss: 0.0415\n",
            "Epoch [26/50], Loss: 0.0351\n",
            "Epoch [27/50], Loss: 0.0346\n",
            "Epoch [28/50], Loss: 0.0314\n",
            "Epoch [29/50], Loss: 0.0328\n",
            "Epoch [30/50], Loss: 0.0320\n",
            "Epoch [31/50], Loss: 0.0297\n",
            "Epoch [32/50], Loss: 0.0289\n",
            "Epoch [33/50], Loss: 0.0293\n",
            "Epoch [34/50], Loss: 0.0264\n",
            "Epoch [35/50], Loss: 0.0220\n",
            "Epoch [36/50], Loss: 0.0220\n",
            "Epoch [37/50], Loss: 0.0170\n",
            "Epoch [38/50], Loss: 0.0162\n",
            "Epoch [39/50], Loss: 0.0174\n",
            "Epoch [40/50], Loss: 0.0246\n",
            "Epoch [41/50], Loss: 0.0223\n",
            "Epoch [42/50], Loss: 0.0175\n",
            "Epoch [43/50], Loss: 0.0138\n",
            "Epoch [44/50], Loss: 0.0157\n",
            "Epoch [45/50], Loss: 0.0199\n",
            "Epoch [46/50], Loss: 0.0140\n",
            "Epoch [47/50], Loss: 0.0098\n",
            "Epoch [48/50], Loss: 0.0091\n",
            "Epoch [49/50], Loss: 0.0113\n",
            "Epoch [50/50], Loss: 0.0087\n",
            "Epoch [1/50], Loss: 0.7014\n",
            "Epoch [2/50], Loss: 0.4435\n",
            "Epoch [3/50], Loss: 0.2885\n",
            "Epoch [4/50], Loss: 0.2035\n",
            "Epoch [5/50], Loss: 0.1533\n",
            "Epoch [6/50], Loss: 0.1408\n",
            "Epoch [7/50], Loss: 0.1173\n",
            "Epoch [8/50], Loss: 0.1052\n",
            "Epoch [9/50], Loss: 0.1176\n",
            "Epoch [10/50], Loss: 0.0785\n",
            "Epoch [11/50], Loss: 0.0802\n",
            "Epoch [12/50], Loss: 0.0796\n",
            "Epoch [13/50], Loss: 0.0723\n",
            "Epoch [14/50], Loss: 0.0566\n",
            "Epoch [15/50], Loss: 0.0491\n",
            "Epoch [16/50], Loss: 0.0556\n",
            "Epoch [17/50], Loss: 0.0497\n",
            "Epoch [18/50], Loss: 0.0423\n",
            "Epoch [19/50], Loss: 0.0561\n",
            "Epoch [20/50], Loss: 0.0438\n",
            "Epoch [21/50], Loss: 0.0267\n",
            "Epoch [22/50], Loss: 0.0228\n",
            "Epoch [23/50], Loss: 0.0204\n",
            "Epoch [24/50], Loss: 0.0224\n",
            "Epoch [25/50], Loss: 0.0224\n",
            "Epoch [26/50], Loss: 0.0200\n",
            "Epoch [27/50], Loss: 0.0260\n",
            "Epoch [28/50], Loss: 0.0172\n",
            "Epoch [29/50], Loss: 0.0157\n",
            "Epoch [30/50], Loss: 0.0141\n",
            "Epoch [31/50], Loss: 0.0128\n",
            "Epoch [32/50], Loss: 0.0127\n",
            "Epoch [33/50], Loss: 0.0165\n",
            "Epoch [34/50], Loss: 0.0174\n",
            "Epoch [35/50], Loss: 0.0147\n",
            "Epoch [36/50], Loss: 0.0117\n",
            "Epoch [37/50], Loss: 0.0084\n",
            "Epoch [38/50], Loss: 0.0073\n",
            "Epoch [39/50], Loss: 0.0061\n",
            "Epoch [40/50], Loss: 0.0060\n",
            "Epoch [41/50], Loss: 0.0056\n",
            "Epoch [42/50], Loss: 0.0054\n",
            "Epoch [43/50], Loss: 0.0051\n",
            "Epoch [44/50], Loss: 0.0044\n",
            "Epoch [45/50], Loss: 0.0045\n",
            "Epoch [46/50], Loss: 0.0039\n",
            "Epoch [47/50], Loss: 0.0038\n",
            "Epoch [48/50], Loss: 0.0038\n",
            "Epoch [49/50], Loss: 0.0037\n",
            "Epoch [50/50], Loss: 0.0034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Comparing Activation Functions\n",
        "activation_functions = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
        "results_activation_functions = []\n",
        "for activation_fn in activation_functions:\n",
        "    accuracy = train_and_evaluate(hidden_layers=[32], activation_fn=activation_fn, learning_rate=0.01, batch_size=32, epochs=50)\n",
        "    results_activation_functions.append({'activation_function': activation_fn, 'accuracy': accuracy})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3imQAXhymwGo",
        "outputId": "a290b086-cbb0-49b4-941e-45f6cb9ab89f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.8493\n",
            "Epoch [2/50], Loss: 0.6064\n",
            "Epoch [3/50], Loss: 0.4875\n",
            "Epoch [4/50], Loss: 0.3719\n",
            "Epoch [5/50], Loss: 0.3093\n",
            "Epoch [6/50], Loss: 0.2529\n",
            "Epoch [7/50], Loss: 0.2179\n",
            "Epoch [8/50], Loss: 0.1921\n",
            "Epoch [9/50], Loss: 0.1689\n",
            "Epoch [10/50], Loss: 0.1556\n",
            "Epoch [11/50], Loss: 0.1399\n",
            "Epoch [12/50], Loss: 0.1327\n",
            "Epoch [13/50], Loss: 0.1178\n",
            "Epoch [14/50], Loss: 0.1138\n",
            "Epoch [15/50], Loss: 0.1080\n",
            "Epoch [16/50], Loss: 0.1044\n",
            "Epoch [17/50], Loss: 0.0962\n",
            "Epoch [18/50], Loss: 0.0900\n",
            "Epoch [19/50], Loss: 0.0802\n",
            "Epoch [20/50], Loss: 0.0797\n",
            "Epoch [21/50], Loss: 0.0782\n",
            "Epoch [22/50], Loss: 0.0692\n",
            "Epoch [23/50], Loss: 0.0655\n",
            "Epoch [24/50], Loss: 0.0633\n",
            "Epoch [25/50], Loss: 0.0620\n",
            "Epoch [26/50], Loss: 0.0632\n",
            "Epoch [27/50], Loss: 0.0579\n",
            "Epoch [28/50], Loss: 0.0571\n",
            "Epoch [29/50], Loss: 0.0537\n",
            "Epoch [30/50], Loss: 0.0544\n",
            "Epoch [31/50], Loss: 0.0545\n",
            "Epoch [32/50], Loss: 0.0475\n",
            "Epoch [33/50], Loss: 0.0469\n",
            "Epoch [34/50], Loss: 0.0518\n",
            "Epoch [35/50], Loss: 0.0441\n",
            "Epoch [36/50], Loss: 0.0411\n",
            "Epoch [37/50], Loss: 0.0449\n",
            "Epoch [38/50], Loss: 0.0445\n",
            "Epoch [39/50], Loss: 0.0420\n",
            "Epoch [40/50], Loss: 0.0493\n",
            "Epoch [41/50], Loss: 0.0425\n",
            "Epoch [42/50], Loss: 0.0411\n",
            "Epoch [43/50], Loss: 0.0375\n",
            "Epoch [44/50], Loss: 0.0434\n",
            "Epoch [45/50], Loss: 0.0356\n",
            "Epoch [46/50], Loss: 0.0320\n",
            "Epoch [47/50], Loss: 0.0418\n",
            "Epoch [48/50], Loss: 0.0359\n",
            "Epoch [49/50], Loss: 0.0300\n",
            "Epoch [50/50], Loss: 0.0299\n",
            "Epoch [1/50], Loss: 0.8207\n",
            "Epoch [2/50], Loss: 0.7196\n",
            "Epoch [3/50], Loss: 0.7116\n",
            "Epoch [4/50], Loss: 0.6906\n",
            "Epoch [5/50], Loss: 0.6934\n",
            "Epoch [6/50], Loss: 0.6710\n",
            "Epoch [7/50], Loss: 0.6692\n",
            "Epoch [8/50], Loss: 0.6576\n",
            "Epoch [9/50], Loss: 0.6389\n",
            "Epoch [10/50], Loss: 0.6236\n",
            "Epoch [11/50], Loss: 0.5913\n",
            "Epoch [12/50], Loss: 0.5566\n",
            "Epoch [13/50], Loss: 0.5122\n",
            "Epoch [14/50], Loss: 0.4645\n",
            "Epoch [15/50], Loss: 0.4342\n",
            "Epoch [16/50], Loss: 0.4097\n",
            "Epoch [17/50], Loss: 0.3802\n",
            "Epoch [18/50], Loss: 0.3571\n",
            "Epoch [19/50], Loss: 0.3415\n",
            "Epoch [20/50], Loss: 0.3254\n",
            "Epoch [21/50], Loss: 0.3081\n",
            "Epoch [22/50], Loss: 0.2965\n",
            "Epoch [23/50], Loss: 0.2796\n",
            "Epoch [24/50], Loss: 0.2661\n",
            "Epoch [25/50], Loss: 0.2556\n",
            "Epoch [26/50], Loss: 0.2431\n",
            "Epoch [27/50], Loss: 0.2327\n",
            "Epoch [28/50], Loss: 0.2226\n",
            "Epoch [29/50], Loss: 0.2176\n",
            "Epoch [30/50], Loss: 0.2013\n",
            "Epoch [31/50], Loss: 0.1893\n",
            "Epoch [32/50], Loss: 0.1746\n",
            "Epoch [33/50], Loss: 0.1681\n",
            "Epoch [34/50], Loss: 0.1604\n",
            "Epoch [35/50], Loss: 0.1511\n",
            "Epoch [36/50], Loss: 0.1461\n",
            "Epoch [37/50], Loss: 0.1425\n",
            "Epoch [38/50], Loss: 0.1294\n",
            "Epoch [39/50], Loss: 0.1233\n",
            "Epoch [40/50], Loss: 0.1212\n",
            "Epoch [41/50], Loss: 0.1123\n",
            "Epoch [42/50], Loss: 0.1101\n",
            "Epoch [43/50], Loss: 0.1037\n",
            "Epoch [44/50], Loss: 0.1011\n",
            "Epoch [45/50], Loss: 0.0973\n",
            "Epoch [46/50], Loss: 0.0919\n",
            "Epoch [47/50], Loss: 0.0894\n",
            "Epoch [48/50], Loss: 0.0834\n",
            "Epoch [49/50], Loss: 0.0807\n",
            "Epoch [50/50], Loss: 0.0767\n",
            "Epoch [1/50], Loss: 0.7974\n",
            "Epoch [2/50], Loss: 0.6776\n",
            "Epoch [3/50], Loss: 0.6148\n",
            "Epoch [4/50], Loss: 0.5396\n",
            "Epoch [5/50], Loss: 0.4802\n",
            "Epoch [6/50], Loss: 0.4406\n",
            "Epoch [7/50], Loss: 0.4344\n",
            "Epoch [8/50], Loss: 0.4054\n",
            "Epoch [9/50], Loss: 0.3769\n",
            "Epoch [10/50], Loss: 0.3488\n",
            "Epoch [11/50], Loss: 0.3430\n",
            "Epoch [12/50], Loss: 0.3212\n",
            "Epoch [13/50], Loss: 0.2931\n",
            "Epoch [14/50], Loss: 0.2759\n",
            "Epoch [15/50], Loss: 0.2628\n",
            "Epoch [16/50], Loss: 0.2423\n",
            "Epoch [17/50], Loss: 0.2376\n",
            "Epoch [18/50], Loss: 0.2189\n",
            "Epoch [19/50], Loss: 0.1912\n",
            "Epoch [20/50], Loss: 0.1842\n",
            "Epoch [21/50], Loss: 0.1645\n",
            "Epoch [22/50], Loss: 0.1463\n",
            "Epoch [23/50], Loss: 0.1315\n",
            "Epoch [24/50], Loss: 0.1241\n",
            "Epoch [25/50], Loss: 0.1103\n",
            "Epoch [26/50], Loss: 0.1054\n",
            "Epoch [27/50], Loss: 0.1011\n",
            "Epoch [28/50], Loss: 0.0945\n",
            "Epoch [29/50], Loss: 0.0861\n",
            "Epoch [30/50], Loss: 0.0827\n",
            "Epoch [31/50], Loss: 0.0776\n",
            "Epoch [32/50], Loss: 0.0698\n",
            "Epoch [33/50], Loss: 0.0652\n",
            "Epoch [34/50], Loss: 0.0635\n",
            "Epoch [35/50], Loss: 0.0619\n",
            "Epoch [36/50], Loss: 0.0575\n",
            "Epoch [37/50], Loss: 0.0547\n",
            "Epoch [38/50], Loss: 0.0523\n",
            "Epoch [39/50], Loss: 0.0512\n",
            "Epoch [40/50], Loss: 0.0482\n",
            "Epoch [41/50], Loss: 0.0457\n",
            "Epoch [42/50], Loss: 0.0440\n",
            "Epoch [43/50], Loss: 0.0414\n",
            "Epoch [44/50], Loss: 0.0405\n",
            "Epoch [45/50], Loss: 0.0384\n",
            "Epoch [46/50], Loss: 0.0377\n",
            "Epoch [47/50], Loss: 0.0352\n",
            "Epoch [48/50], Loss: 0.0339\n",
            "Epoch [49/50], Loss: 0.0357\n",
            "Epoch [50/50], Loss: 0.0344\n",
            "Epoch [1/50], Loss: 1.1225\n",
            "Epoch [2/50], Loss: 0.8218\n",
            "Epoch [3/50], Loss: 0.7302\n",
            "Epoch [4/50], Loss: 0.6344\n",
            "Epoch [5/50], Loss: 0.5543\n",
            "Epoch [6/50], Loss: 0.4733\n",
            "Epoch [7/50], Loss: 0.4148\n",
            "Epoch [8/50], Loss: 0.3696\n",
            "Epoch [9/50], Loss: 0.3504\n",
            "Epoch [10/50], Loss: 0.3148\n",
            "Epoch [11/50], Loss: 0.2990\n",
            "Epoch [12/50], Loss: 0.2834\n",
            "Epoch [13/50], Loss: 0.2702\n",
            "Epoch [14/50], Loss: 0.2634\n",
            "Epoch [15/50], Loss: 0.2534\n",
            "Epoch [16/50], Loss: 0.2429\n",
            "Epoch [17/50], Loss: 0.2372\n",
            "Epoch [18/50], Loss: 0.2289\n",
            "Epoch [19/50], Loss: 0.2256\n",
            "Epoch [20/50], Loss: 0.2215\n",
            "Epoch [21/50], Loss: 0.2134\n",
            "Epoch [22/50], Loss: 0.2136\n",
            "Epoch [23/50], Loss: 0.2057\n",
            "Epoch [24/50], Loss: 0.2014\n",
            "Epoch [25/50], Loss: 0.1927\n",
            "Epoch [26/50], Loss: 0.1920\n",
            "Epoch [27/50], Loss: 0.1907\n",
            "Epoch [28/50], Loss: 0.1845\n",
            "Epoch [29/50], Loss: 0.1805\n",
            "Epoch [30/50], Loss: 0.1816\n",
            "Epoch [31/50], Loss: 0.1740\n",
            "Epoch [32/50], Loss: 0.1733\n",
            "Epoch [33/50], Loss: 0.1713\n",
            "Epoch [34/50], Loss: 0.1667\n",
            "Epoch [35/50], Loss: 0.1673\n",
            "Epoch [36/50], Loss: 0.1599\n",
            "Epoch [37/50], Loss: 0.1605\n",
            "Epoch [38/50], Loss: 0.1578\n",
            "Epoch [39/50], Loss: 0.1526\n",
            "Epoch [40/50], Loss: 0.1525\n",
            "Epoch [41/50], Loss: 0.1516\n",
            "Epoch [42/50], Loss: 0.1498\n",
            "Epoch [43/50], Loss: 0.1446\n",
            "Epoch [44/50], Loss: 0.1453\n",
            "Epoch [45/50], Loss: 0.1406\n",
            "Epoch [46/50], Loss: 0.1365\n",
            "Epoch [47/50], Loss: 0.1375\n",
            "Epoch [48/50], Loss: 0.1338\n",
            "Epoch [49/50], Loss: 0.1378\n",
            "Epoch [50/50], Loss: 0.1304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: Comparing Epochs\n",
        "epochs_options = [1, 10, 25, 50, 100, 250]\n",
        "results_epochs = []\n",
        "for epochs in epochs_options:\n",
        "    accuracy = train_and_evaluate(hidden_layers=[32], activation_fn='relu', learning_rate=0.01, batch_size=32, epochs=epochs)\n",
        "    results_epochs.append({'epochs': epochs, 'accuracy': accuracy})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da-Xj_tJmydh",
        "outputId": "5ceefa93-f808-42c9-ba87-efce1acf2aec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.7964\n",
            "Epoch [1/10], Loss: 0.7931\n",
            "Epoch [2/10], Loss: 0.5845\n",
            "Epoch [3/10], Loss: 0.4588\n",
            "Epoch [4/10], Loss: 0.3577\n",
            "Epoch [5/10], Loss: 0.3025\n",
            "Epoch [6/10], Loss: 0.2544\n",
            "Epoch [7/10], Loss: 0.2244\n",
            "Epoch [8/10], Loss: 0.1870\n",
            "Epoch [9/10], Loss: 0.1668\n",
            "Epoch [10/10], Loss: 0.1489\n",
            "Epoch [1/25], Loss: 0.8102\n",
            "Epoch [2/25], Loss: 0.6044\n",
            "Epoch [3/25], Loss: 0.4855\n",
            "Epoch [4/25], Loss: 0.3771\n",
            "Epoch [5/25], Loss: 0.3018\n",
            "Epoch [6/25], Loss: 0.2682\n",
            "Epoch [7/25], Loss: 0.2189\n",
            "Epoch [8/25], Loss: 0.1975\n",
            "Epoch [9/25], Loss: 0.1720\n",
            "Epoch [10/25], Loss: 0.1516\n",
            "Epoch [11/25], Loss: 0.1421\n",
            "Epoch [12/25], Loss: 0.1430\n",
            "Epoch [13/25], Loss: 0.1228\n",
            "Epoch [14/25], Loss: 0.1123\n",
            "Epoch [15/25], Loss: 0.1065\n",
            "Epoch [16/25], Loss: 0.1056\n",
            "Epoch [17/25], Loss: 0.0933\n",
            "Epoch [18/25], Loss: 0.0958\n",
            "Epoch [19/25], Loss: 0.0908\n",
            "Epoch [20/25], Loss: 0.0813\n",
            "Epoch [21/25], Loss: 0.0858\n",
            "Epoch [22/25], Loss: 0.0921\n",
            "Epoch [23/25], Loss: 0.0741\n",
            "Epoch [24/25], Loss: 0.0689\n",
            "Epoch [25/25], Loss: 0.0649\n",
            "Epoch [1/50], Loss: 0.8426\n",
            "Epoch [2/50], Loss: 0.6236\n",
            "Epoch [3/50], Loss: 0.5339\n",
            "Epoch [4/50], Loss: 0.4153\n",
            "Epoch [5/50], Loss: 0.3245\n",
            "Epoch [6/50], Loss: 0.2714\n",
            "Epoch [7/50], Loss: 0.2376\n",
            "Epoch [8/50], Loss: 0.2019\n",
            "Epoch [9/50], Loss: 0.1778\n",
            "Epoch [10/50], Loss: 0.1577\n",
            "Epoch [11/50], Loss: 0.1414\n",
            "Epoch [12/50], Loss: 0.1300\n",
            "Epoch [13/50], Loss: 0.1168\n",
            "Epoch [14/50], Loss: 0.1073\n",
            "Epoch [15/50], Loss: 0.1082\n",
            "Epoch [16/50], Loss: 0.1001\n",
            "Epoch [17/50], Loss: 0.0898\n",
            "Epoch [18/50], Loss: 0.0825\n",
            "Epoch [19/50], Loss: 0.0809\n",
            "Epoch [20/50], Loss: 0.0715\n",
            "Epoch [21/50], Loss: 0.0706\n",
            "Epoch [22/50], Loss: 0.0784\n",
            "Epoch [23/50], Loss: 0.0732\n",
            "Epoch [24/50], Loss: 0.0610\n",
            "Epoch [25/50], Loss: 0.0572\n",
            "Epoch [26/50], Loss: 0.0525\n",
            "Epoch [27/50], Loss: 0.0546\n",
            "Epoch [28/50], Loss: 0.0558\n",
            "Epoch [29/50], Loss: 0.0460\n",
            "Epoch [30/50], Loss: 0.0469\n",
            "Epoch [31/50], Loss: 0.0459\n",
            "Epoch [32/50], Loss: 0.0462\n",
            "Epoch [33/50], Loss: 0.0390\n",
            "Epoch [34/50], Loss: 0.0380\n",
            "Epoch [35/50], Loss: 0.0376\n",
            "Epoch [36/50], Loss: 0.0381\n",
            "Epoch [37/50], Loss: 0.0402\n",
            "Epoch [38/50], Loss: 0.0323\n",
            "Epoch [39/50], Loss: 0.0327\n",
            "Epoch [40/50], Loss: 0.0311\n",
            "Epoch [41/50], Loss: 0.0257\n",
            "Epoch [42/50], Loss: 0.0254\n",
            "Epoch [43/50], Loss: 0.0248\n",
            "Epoch [44/50], Loss: 0.0244\n",
            "Epoch [45/50], Loss: 0.0241\n",
            "Epoch [46/50], Loss: 0.0235\n",
            "Epoch [47/50], Loss: 0.0251\n",
            "Epoch [48/50], Loss: 0.0206\n",
            "Epoch [49/50], Loss: 0.0204\n",
            "Epoch [50/50], Loss: 0.0273\n",
            "Epoch [1/100], Loss: 0.7960\n",
            "Epoch [2/100], Loss: 0.5882\n",
            "Epoch [3/100], Loss: 0.4598\n",
            "Epoch [4/100], Loss: 0.3514\n",
            "Epoch [5/100], Loss: 0.2862\n",
            "Epoch [6/100], Loss: 0.2362\n",
            "Epoch [7/100], Loss: 0.2053\n",
            "Epoch [8/100], Loss: 0.1791\n",
            "Epoch [9/100], Loss: 0.1603\n",
            "Epoch [10/100], Loss: 0.1446\n",
            "Epoch [11/100], Loss: 0.1312\n",
            "Epoch [12/100], Loss: 0.1103\n",
            "Epoch [13/100], Loss: 0.1053\n",
            "Epoch [14/100], Loss: 0.0974\n",
            "Epoch [15/100], Loss: 0.0898\n",
            "Epoch [16/100], Loss: 0.0868\n",
            "Epoch [17/100], Loss: 0.0837\n",
            "Epoch [18/100], Loss: 0.0743\n",
            "Epoch [19/100], Loss: 0.0721\n",
            "Epoch [20/100], Loss: 0.0701\n",
            "Epoch [21/100], Loss: 0.0703\n",
            "Epoch [22/100], Loss: 0.0658\n",
            "Epoch [23/100], Loss: 0.0607\n",
            "Epoch [24/100], Loss: 0.0612\n",
            "Epoch [25/100], Loss: 0.0579\n",
            "Epoch [26/100], Loss: 0.0577\n",
            "Epoch [27/100], Loss: 0.0590\n",
            "Epoch [28/100], Loss: 0.0527\n",
            "Epoch [29/100], Loss: 0.0492\n",
            "Epoch [30/100], Loss: 0.0481\n",
            "Epoch [31/100], Loss: 0.0480\n",
            "Epoch [32/100], Loss: 0.0476\n",
            "Epoch [33/100], Loss: 0.0433\n",
            "Epoch [34/100], Loss: 0.0454\n",
            "Epoch [35/100], Loss: 0.0419\n",
            "Epoch [36/100], Loss: 0.0399\n",
            "Epoch [37/100], Loss: 0.0479\n",
            "Epoch [38/100], Loss: 0.0456\n",
            "Epoch [39/100], Loss: 0.0354\n",
            "Epoch [40/100], Loss: 0.0361\n",
            "Epoch [41/100], Loss: 0.0349\n",
            "Epoch [42/100], Loss: 0.0335\n",
            "Epoch [43/100], Loss: 0.0301\n",
            "Epoch [44/100], Loss: 0.0302\n",
            "Epoch [45/100], Loss: 0.0282\n",
            "Epoch [46/100], Loss: 0.0335\n",
            "Epoch [47/100], Loss: 0.0304\n",
            "Epoch [48/100], Loss: 0.0294\n",
            "Epoch [49/100], Loss: 0.0256\n",
            "Epoch [50/100], Loss: 0.0302\n",
            "Epoch [51/100], Loss: 0.0264\n",
            "Epoch [52/100], Loss: 0.0264\n",
            "Epoch [53/100], Loss: 0.0265\n",
            "Epoch [54/100], Loss: 0.0337\n",
            "Epoch [55/100], Loss: 0.0485\n",
            "Epoch [56/100], Loss: 0.0267\n",
            "Epoch [57/100], Loss: 0.0308\n",
            "Epoch [58/100], Loss: 0.0238\n",
            "Epoch [59/100], Loss: 0.0291\n",
            "Epoch [60/100], Loss: 0.0283\n",
            "Epoch [61/100], Loss: 0.0212\n",
            "Epoch [62/100], Loss: 0.0231\n",
            "Epoch [63/100], Loss: 0.0189\n",
            "Epoch [64/100], Loss: 0.0170\n",
            "Epoch [65/100], Loss: 0.0193\n",
            "Epoch [66/100], Loss: 0.0194\n",
            "Epoch [67/100], Loss: 0.0218\n",
            "Epoch [68/100], Loss: 0.0211\n",
            "Epoch [69/100], Loss: 0.0152\n",
            "Epoch [70/100], Loss: 0.0184\n",
            "Epoch [71/100], Loss: 0.0161\n",
            "Epoch [72/100], Loss: 0.0173\n",
            "Epoch [73/100], Loss: 0.0156\n",
            "Epoch [74/100], Loss: 0.0139\n",
            "Epoch [75/100], Loss: 0.0136\n",
            "Epoch [76/100], Loss: 0.0198\n",
            "Epoch [77/100], Loss: 0.0166\n",
            "Epoch [78/100], Loss: 0.0300\n",
            "Epoch [79/100], Loss: 0.0148\n",
            "Epoch [80/100], Loss: 0.0132\n",
            "Epoch [81/100], Loss: 0.0133\n",
            "Epoch [82/100], Loss: 0.0193\n",
            "Epoch [83/100], Loss: 0.0142\n",
            "Epoch [84/100], Loss: 0.0181\n",
            "Epoch [85/100], Loss: 0.0240\n",
            "Epoch [86/100], Loss: 0.0242\n",
            "Epoch [87/100], Loss: 0.0239\n",
            "Epoch [88/100], Loss: 0.0185\n",
            "Epoch [89/100], Loss: 0.0122\n",
            "Epoch [90/100], Loss: 0.0158\n",
            "Epoch [91/100], Loss: 0.0103\n",
            "Epoch [92/100], Loss: 0.0106\n",
            "Epoch [93/100], Loss: 0.0120\n",
            "Epoch [94/100], Loss: 0.0092\n",
            "Epoch [95/100], Loss: 0.0090\n",
            "Epoch [96/100], Loss: 0.0084\n",
            "Epoch [97/100], Loss: 0.0090\n",
            "Epoch [98/100], Loss: 0.0122\n",
            "Epoch [99/100], Loss: 0.0102\n",
            "Epoch [100/100], Loss: 0.0088\n",
            "Epoch [1/250], Loss: 0.7887\n",
            "Epoch [2/250], Loss: 0.5552\n",
            "Epoch [3/250], Loss: 0.4202\n",
            "Epoch [4/250], Loss: 0.3268\n",
            "Epoch [5/250], Loss: 0.2739\n",
            "Epoch [6/250], Loss: 0.2391\n",
            "Epoch [7/250], Loss: 0.2062\n",
            "Epoch [8/250], Loss: 0.1839\n",
            "Epoch [9/250], Loss: 0.1591\n",
            "Epoch [10/250], Loss: 0.1482\n",
            "Epoch [11/250], Loss: 0.1324\n",
            "Epoch [12/250], Loss: 0.1200\n",
            "Epoch [13/250], Loss: 0.1077\n",
            "Epoch [14/250], Loss: 0.1011\n",
            "Epoch [15/250], Loss: 0.0919\n",
            "Epoch [16/250], Loss: 0.0858\n",
            "Epoch [17/250], Loss: 0.0791\n",
            "Epoch [18/250], Loss: 0.0822\n",
            "Epoch [19/250], Loss: 0.0691\n",
            "Epoch [20/250], Loss: 0.0700\n",
            "Epoch [21/250], Loss: 0.0630\n",
            "Epoch [22/250], Loss: 0.0616\n",
            "Epoch [23/250], Loss: 0.0621\n",
            "Epoch [24/250], Loss: 0.0592\n",
            "Epoch [25/250], Loss: 0.0572\n",
            "Epoch [26/250], Loss: 0.0567\n",
            "Epoch [27/250], Loss: 0.0491\n",
            "Epoch [28/250], Loss: 0.0495\n",
            "Epoch [29/250], Loss: 0.0537\n",
            "Epoch [30/250], Loss: 0.0421\n",
            "Epoch [31/250], Loss: 0.0448\n",
            "Epoch [32/250], Loss: 0.0424\n",
            "Epoch [33/250], Loss: 0.0410\n",
            "Epoch [34/250], Loss: 0.0398\n",
            "Epoch [35/250], Loss: 0.0374\n",
            "Epoch [36/250], Loss: 0.0341\n",
            "Epoch [37/250], Loss: 0.0374\n",
            "Epoch [38/250], Loss: 0.0355\n",
            "Epoch [39/250], Loss: 0.0306\n",
            "Epoch [40/250], Loss: 0.0275\n",
            "Epoch [41/250], Loss: 0.0289\n",
            "Epoch [42/250], Loss: 0.0288\n",
            "Epoch [43/250], Loss: 0.0308\n",
            "Epoch [44/250], Loss: 0.0262\n",
            "Epoch [45/250], Loss: 0.0265\n",
            "Epoch [46/250], Loss: 0.0226\n",
            "Epoch [47/250], Loss: 0.0260\n",
            "Epoch [48/250], Loss: 0.0222\n",
            "Epoch [49/250], Loss: 0.0212\n",
            "Epoch [50/250], Loss: 0.0216\n",
            "Epoch [51/250], Loss: 0.0177\n",
            "Epoch [52/250], Loss: 0.0196\n",
            "Epoch [53/250], Loss: 0.0167\n",
            "Epoch [54/250], Loss: 0.0166\n",
            "Epoch [55/250], Loss: 0.0188\n",
            "Epoch [56/250], Loss: 0.0153\n",
            "Epoch [57/250], Loss: 0.0155\n",
            "Epoch [58/250], Loss: 0.0139\n",
            "Epoch [59/250], Loss: 0.0142\n",
            "Epoch [60/250], Loss: 0.0150\n",
            "Epoch [61/250], Loss: 0.0216\n",
            "Epoch [62/250], Loss: 0.0376\n",
            "Epoch [63/250], Loss: 0.0220\n",
            "Epoch [64/250], Loss: 0.0156\n",
            "Epoch [65/250], Loss: 0.0179\n",
            "Epoch [66/250], Loss: 0.0171\n",
            "Epoch [67/250], Loss: 0.0113\n",
            "Epoch [68/250], Loss: 0.0099\n",
            "Epoch [69/250], Loss: 0.0092\n",
            "Epoch [70/250], Loss: 0.0090\n",
            "Epoch [71/250], Loss: 0.0117\n",
            "Epoch [72/250], Loss: 0.0283\n",
            "Epoch [73/250], Loss: 0.0227\n",
            "Epoch [74/250], Loss: 0.0135\n",
            "Epoch [75/250], Loss: 0.0078\n",
            "Epoch [76/250], Loss: 0.0081\n",
            "Epoch [77/250], Loss: 0.0066\n",
            "Epoch [78/250], Loss: 0.0060\n",
            "Epoch [79/250], Loss: 0.0080\n",
            "Epoch [80/250], Loss: 0.0065\n",
            "Epoch [81/250], Loss: 0.0057\n",
            "Epoch [82/250], Loss: 0.0055\n",
            "Epoch [83/250], Loss: 0.0062\n",
            "Epoch [84/250], Loss: 0.0060\n",
            "Epoch [85/250], Loss: 0.0049\n",
            "Epoch [86/250], Loss: 0.0051\n",
            "Epoch [87/250], Loss: 0.0057\n",
            "Epoch [88/250], Loss: 0.0049\n",
            "Epoch [89/250], Loss: 0.0050\n",
            "Epoch [90/250], Loss: 0.0046\n",
            "Epoch [91/250], Loss: 0.0045\n",
            "Epoch [92/250], Loss: 0.0044\n",
            "Epoch [93/250], Loss: 0.0043\n",
            "Epoch [94/250], Loss: 0.0041\n",
            "Epoch [95/250], Loss: 0.0041\n",
            "Epoch [96/250], Loss: 0.0043\n",
            "Epoch [97/250], Loss: 0.0037\n",
            "Epoch [98/250], Loss: 0.0035\n",
            "Epoch [99/250], Loss: 0.0036\n",
            "Epoch [100/250], Loss: 0.0035\n",
            "Epoch [101/250], Loss: 0.0035\n",
            "Epoch [102/250], Loss: 0.0032\n",
            "Epoch [103/250], Loss: 0.0036\n",
            "Epoch [104/250], Loss: 0.0031\n",
            "Epoch [105/250], Loss: 0.0031\n",
            "Epoch [106/250], Loss: 0.0030\n",
            "Epoch [107/250], Loss: 0.0029\n",
            "Epoch [108/250], Loss: 0.0028\n",
            "Epoch [109/250], Loss: 0.0029\n",
            "Epoch [110/250], Loss: 0.0027\n",
            "Epoch [111/250], Loss: 0.0032\n",
            "Epoch [112/250], Loss: 0.0030\n",
            "Epoch [113/250], Loss: 0.0029\n",
            "Epoch [114/250], Loss: 0.0030\n",
            "Epoch [115/250], Loss: 0.0026\n",
            "Epoch [116/250], Loss: 0.0021\n",
            "Epoch [117/250], Loss: 0.0020\n",
            "Epoch [118/250], Loss: 0.0020\n",
            "Epoch [119/250], Loss: 0.0019\n",
            "Epoch [120/250], Loss: 0.0019\n",
            "Epoch [121/250], Loss: 0.0019\n",
            "Epoch [122/250], Loss: 0.0019\n",
            "Epoch [123/250], Loss: 0.0018\n",
            "Epoch [124/250], Loss: 0.0018\n",
            "Epoch [125/250], Loss: 0.0018\n",
            "Epoch [126/250], Loss: 0.0020\n",
            "Epoch [127/250], Loss: 0.0032\n",
            "Epoch [128/250], Loss: 0.0440\n",
            "Epoch [129/250], Loss: 0.1166\n",
            "Epoch [130/250], Loss: 0.0244\n",
            "Epoch [131/250], Loss: 0.0038\n",
            "Epoch [132/250], Loss: 0.0025\n",
            "Epoch [133/250], Loss: 0.0021\n",
            "Epoch [134/250], Loss: 0.0017\n",
            "Epoch [135/250], Loss: 0.0017\n",
            "Epoch [136/250], Loss: 0.0017\n",
            "Epoch [137/250], Loss: 0.0016\n",
            "Epoch [138/250], Loss: 0.0015\n",
            "Epoch [139/250], Loss: 0.0015\n",
            "Epoch [140/250], Loss: 0.0015\n",
            "Epoch [141/250], Loss: 0.0015\n",
            "Epoch [142/250], Loss: 0.0014\n",
            "Epoch [143/250], Loss: 0.0014\n",
            "Epoch [144/250], Loss: 0.0013\n",
            "Epoch [145/250], Loss: 0.0013\n",
            "Epoch [146/250], Loss: 0.0014\n",
            "Epoch [147/250], Loss: 0.0013\n",
            "Epoch [148/250], Loss: 0.0013\n",
            "Epoch [149/250], Loss: 0.0012\n",
            "Epoch [150/250], Loss: 0.0013\n",
            "Epoch [151/250], Loss: 0.0012\n",
            "Epoch [152/250], Loss: 0.0013\n",
            "Epoch [153/250], Loss: 0.0012\n",
            "Epoch [154/250], Loss: 0.0011\n",
            "Epoch [155/250], Loss: 0.0012\n",
            "Epoch [156/250], Loss: 0.0012\n",
            "Epoch [157/250], Loss: 0.0011\n",
            "Epoch [158/250], Loss: 0.0012\n",
            "Epoch [159/250], Loss: 0.0011\n",
            "Epoch [160/250], Loss: 0.0010\n",
            "Epoch [161/250], Loss: 0.0011\n",
            "Epoch [162/250], Loss: 0.0010\n",
            "Epoch [163/250], Loss: 0.0011\n",
            "Epoch [164/250], Loss: 0.0010\n",
            "Epoch [165/250], Loss: 0.0009\n",
            "Epoch [166/250], Loss: 0.0010\n",
            "Epoch [167/250], Loss: 0.0009\n",
            "Epoch [168/250], Loss: 0.0009\n",
            "Epoch [169/250], Loss: 0.0010\n",
            "Epoch [170/250], Loss: 0.0010\n",
            "Epoch [171/250], Loss: 0.0009\n",
            "Epoch [172/250], Loss: 0.0009\n",
            "Epoch [173/250], Loss: 0.0008\n",
            "Epoch [174/250], Loss: 0.0008\n",
            "Epoch [175/250], Loss: 0.0008\n",
            "Epoch [176/250], Loss: 0.0009\n",
            "Epoch [177/250], Loss: 0.0009\n",
            "Epoch [178/250], Loss: 0.0008\n",
            "Epoch [179/250], Loss: 0.0008\n",
            "Epoch [180/250], Loss: 0.0008\n",
            "Epoch [181/250], Loss: 0.0008\n",
            "Epoch [182/250], Loss: 0.0008\n",
            "Epoch [183/250], Loss: 0.0007\n",
            "Epoch [184/250], Loss: 0.0007\n",
            "Epoch [185/250], Loss: 0.0007\n",
            "Epoch [186/250], Loss: 0.0008\n",
            "Epoch [187/250], Loss: 0.0009\n",
            "Epoch [188/250], Loss: 0.0006\n",
            "Epoch [189/250], Loss: 0.0006\n",
            "Epoch [190/250], Loss: 0.0007\n",
            "Epoch [191/250], Loss: 0.0006\n",
            "Epoch [192/250], Loss: 0.0006\n",
            "Epoch [193/250], Loss: 0.0006\n",
            "Epoch [194/250], Loss: 0.0006\n",
            "Epoch [195/250], Loss: 0.0006\n",
            "Epoch [196/250], Loss: 0.0006\n",
            "Epoch [197/250], Loss: 0.0006\n",
            "Epoch [198/250], Loss: 0.0006\n",
            "Epoch [199/250], Loss: 0.0006\n",
            "Epoch [200/250], Loss: 0.0006\n",
            "Epoch [201/250], Loss: 0.0005\n",
            "Epoch [202/250], Loss: 0.0005\n",
            "Epoch [203/250], Loss: 0.0005\n",
            "Epoch [204/250], Loss: 0.0005\n",
            "Epoch [205/250], Loss: 0.0005\n",
            "Epoch [206/250], Loss: 0.0005\n",
            "Epoch [207/250], Loss: 0.0005\n",
            "Epoch [208/250], Loss: 0.0005\n",
            "Epoch [209/250], Loss: 0.0005\n",
            "Epoch [210/250], Loss: 0.0005\n",
            "Epoch [211/250], Loss: 0.0005\n",
            "Epoch [212/250], Loss: 0.0004\n",
            "Epoch [213/250], Loss: 0.0004\n",
            "Epoch [214/250], Loss: 0.0004\n",
            "Epoch [215/250], Loss: 0.0004\n",
            "Epoch [216/250], Loss: 0.0004\n",
            "Epoch [217/250], Loss: 0.0004\n",
            "Epoch [218/250], Loss: 0.0004\n",
            "Epoch [219/250], Loss: 0.0004\n",
            "Epoch [220/250], Loss: 0.0004\n",
            "Epoch [221/250], Loss: 0.0004\n",
            "Epoch [222/250], Loss: 0.0004\n",
            "Epoch [223/250], Loss: 0.0004\n",
            "Epoch [224/250], Loss: 0.0004\n",
            "Epoch [225/250], Loss: 0.0004\n",
            "Epoch [226/250], Loss: 0.0005\n",
            "Epoch [227/250], Loss: 0.0013\n",
            "Epoch [228/250], Loss: 0.1053\n",
            "Epoch [229/250], Loss: 0.0855\n",
            "Epoch [230/250], Loss: 0.0188\n",
            "Epoch [231/250], Loss: 0.0058\n",
            "Epoch [232/250], Loss: 0.0011\n",
            "Epoch [233/250], Loss: 0.0007\n",
            "Epoch [234/250], Loss: 0.0006\n",
            "Epoch [235/250], Loss: 0.0006\n",
            "Epoch [236/250], Loss: 0.0005\n",
            "Epoch [237/250], Loss: 0.0005\n",
            "Epoch [238/250], Loss: 0.0005\n",
            "Epoch [239/250], Loss: 0.0005\n",
            "Epoch [240/250], Loss: 0.0005\n",
            "Epoch [241/250], Loss: 0.0005\n",
            "Epoch [242/250], Loss: 0.0004\n",
            "Epoch [243/250], Loss: 0.0004\n",
            "Epoch [244/250], Loss: 0.0004\n",
            "Epoch [245/250], Loss: 0.0004\n",
            "Epoch [246/250], Loss: 0.0005\n",
            "Epoch [247/250], Loss: 0.0004\n",
            "Epoch [248/250], Loss: 0.0004\n",
            "Epoch [249/250], Loss: 0.0005\n",
            "Epoch [250/250], Loss: 0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 4: Comparing Learning Rates\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "results_learning_rates = []\n",
        "for learning_rate in learning_rates:\n",
        "    accuracy = train_and_evaluate(hidden_layers=[32], activation_fn='relu', learning_rate=learning_rate, batch_size=32, epochs=50)\n",
        "    results_learning_rates.append({'learning_rate': learning_rate, 'accuracy': accuracy})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ThrxQWdm0MF",
        "outputId": "1a80b179-a095-479f-bc4c-a3820402d165"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 339.3041\n",
            "Epoch [2/50], Loss: 5.7993\n",
            "Epoch [3/50], Loss: 1.1252\n",
            "Epoch [4/50], Loss: 0.9100\n",
            "Epoch [5/50], Loss: 1.0542\n",
            "Epoch [6/50], Loss: 0.8951\n",
            "Epoch [7/50], Loss: 0.9757\n",
            "Epoch [8/50], Loss: 1.0851\n",
            "Epoch [9/50], Loss: 1.1045\n",
            "Epoch [10/50], Loss: 0.8065\n",
            "Epoch [11/50], Loss: 0.9237\n",
            "Epoch [12/50], Loss: 1.1141\n",
            "Epoch [13/50], Loss: 1.5351\n",
            "Epoch [14/50], Loss: 1.7963\n",
            "Epoch [15/50], Loss: 1.2554\n",
            "Epoch [16/50], Loss: 0.9578\n",
            "Epoch [17/50], Loss: 1.1620\n",
            "Epoch [18/50], Loss: 1.0803\n",
            "Epoch [19/50], Loss: 1.3108\n",
            "Epoch [20/50], Loss: 0.8163\n",
            "Epoch [21/50], Loss: 1.0439\n",
            "Epoch [22/50], Loss: 0.9306\n",
            "Epoch [23/50], Loss: 0.8911\n",
            "Epoch [24/50], Loss: 0.9575\n",
            "Epoch [25/50], Loss: 0.9491\n",
            "Epoch [26/50], Loss: 0.9807\n",
            "Epoch [27/50], Loss: 1.4206\n",
            "Epoch [28/50], Loss: 0.9755\n",
            "Epoch [29/50], Loss: 0.9081\n",
            "Epoch [30/50], Loss: 1.5156\n",
            "Epoch [31/50], Loss: 0.8772\n",
            "Epoch [32/50], Loss: 0.8990\n",
            "Epoch [33/50], Loss: 1.4534\n",
            "Epoch [34/50], Loss: 1.0000\n",
            "Epoch [35/50], Loss: 0.9886\n",
            "Epoch [36/50], Loss: 0.8735\n",
            "Epoch [37/50], Loss: 0.8840\n",
            "Epoch [38/50], Loss: 1.1278\n",
            "Epoch [39/50], Loss: 29.9819\n",
            "Epoch [40/50], Loss: 873.9847\n",
            "Epoch [41/50], Loss: 24.2116\n",
            "Epoch [42/50], Loss: 5.3993\n",
            "Epoch [43/50], Loss: 0.9386\n",
            "Epoch [44/50], Loss: 1.1257\n",
            "Epoch [45/50], Loss: 1.1243\n",
            "Epoch [46/50], Loss: 1.1502\n",
            "Epoch [47/50], Loss: 1.0429\n",
            "Epoch [48/50], Loss: 1.3737\n",
            "Epoch [49/50], Loss: 1.0943\n",
            "Epoch [50/50], Loss: 0.9722\n",
            "Epoch [1/50], Loss: 3.3197\n",
            "Epoch [2/50], Loss: 0.7184\n",
            "Epoch [3/50], Loss: 0.7395\n",
            "Epoch [4/50], Loss: 0.7232\n",
            "Epoch [5/50], Loss: 0.7172\n",
            "Epoch [6/50], Loss: 0.7156\n",
            "Epoch [7/50], Loss: 0.6927\n",
            "Epoch [8/50], Loss: 0.7136\n",
            "Epoch [9/50], Loss: 0.7027\n",
            "Epoch [10/50], Loss: 0.7854\n",
            "Epoch [11/50], Loss: 0.7925\n",
            "Epoch [12/50], Loss: 0.7758\n",
            "Epoch [13/50], Loss: 0.7207\n",
            "Epoch [14/50], Loss: 0.7137\n",
            "Epoch [15/50], Loss: 0.7131\n",
            "Epoch [16/50], Loss: 0.7178\n",
            "Epoch [17/50], Loss: 0.7513\n",
            "Epoch [18/50], Loss: 0.7607\n",
            "Epoch [19/50], Loss: 0.7148\n",
            "Epoch [20/50], Loss: 0.7044\n",
            "Epoch [21/50], Loss: 0.7258\n",
            "Epoch [22/50], Loss: 0.7243\n",
            "Epoch [23/50], Loss: 0.6979\n",
            "Epoch [24/50], Loss: 0.7096\n",
            "Epoch [25/50], Loss: 0.7305\n",
            "Epoch [26/50], Loss: 0.7429\n",
            "Epoch [27/50], Loss: 0.7545\n",
            "Epoch [28/50], Loss: 0.7172\n",
            "Epoch [29/50], Loss: 0.7175\n",
            "Epoch [30/50], Loss: 0.7303\n",
            "Epoch [31/50], Loss: 0.7043\n",
            "Epoch [32/50], Loss: 0.7235\n",
            "Epoch [33/50], Loss: 0.7259\n",
            "Epoch [34/50], Loss: 0.7358\n",
            "Epoch [35/50], Loss: 0.7323\n",
            "Epoch [36/50], Loss: 0.7383\n",
            "Epoch [37/50], Loss: 0.7291\n",
            "Epoch [38/50], Loss: 0.7051\n",
            "Epoch [39/50], Loss: 0.7120\n",
            "Epoch [40/50], Loss: 0.7257\n",
            "Epoch [41/50], Loss: 0.7065\n",
            "Epoch [42/50], Loss: 0.7548\n",
            "Epoch [43/50], Loss: 0.7637\n",
            "Epoch [44/50], Loss: 0.7240\n",
            "Epoch [45/50], Loss: 0.7145\n",
            "Epoch [46/50], Loss: 0.7128\n",
            "Epoch [47/50], Loss: 0.7171\n",
            "Epoch [48/50], Loss: 0.7207\n",
            "Epoch [49/50], Loss: 0.7172\n",
            "Epoch [50/50], Loss: 0.7097\n",
            "Epoch [1/50], Loss: 0.5441\n",
            "Epoch [2/50], Loss: 0.2991\n",
            "Epoch [3/50], Loss: 0.2634\n",
            "Epoch [4/50], Loss: 0.2750\n",
            "Epoch [5/50], Loss: 0.1876\n",
            "Epoch [6/50], Loss: 0.1420\n",
            "Epoch [7/50], Loss: 0.2574\n",
            "Epoch [8/50], Loss: 0.1741\n",
            "Epoch [9/50], Loss: 0.1626\n",
            "Epoch [10/50], Loss: 0.1637\n",
            "Epoch [11/50], Loss: 0.1568\n",
            "Epoch [12/50], Loss: 0.1415\n",
            "Epoch [13/50], Loss: 0.1448\n",
            "Epoch [14/50], Loss: 0.1886\n",
            "Epoch [15/50], Loss: 0.1453\n",
            "Epoch [16/50], Loss: 0.0886\n",
            "Epoch [17/50], Loss: 0.0861\n",
            "Epoch [18/50], Loss: 0.0879\n",
            "Epoch [19/50], Loss: 0.1818\n",
            "Epoch [20/50], Loss: 0.2658\n",
            "Epoch [21/50], Loss: 0.1994\n",
            "Epoch [22/50], Loss: 0.1225\n",
            "Epoch [23/50], Loss: 0.0603\n",
            "Epoch [24/50], Loss: 0.1208\n",
            "Epoch [25/50], Loss: 0.0662\n",
            "Epoch [26/50], Loss: 0.0388\n",
            "Epoch [27/50], Loss: 0.0485\n",
            "Epoch [28/50], Loss: 0.0733\n",
            "Epoch [29/50], Loss: 0.1544\n",
            "Epoch [30/50], Loss: 0.1612\n",
            "Epoch [31/50], Loss: 0.1277\n",
            "Epoch [32/50], Loss: 0.1057\n",
            "Epoch [33/50], Loss: 0.0616\n",
            "Epoch [34/50], Loss: 0.1101\n",
            "Epoch [35/50], Loss: 0.1430\n",
            "Epoch [36/50], Loss: 0.0604\n",
            "Epoch [37/50], Loss: 0.0436\n",
            "Epoch [38/50], Loss: 0.0287\n",
            "Epoch [39/50], Loss: 0.0151\n",
            "Epoch [40/50], Loss: 0.0179\n",
            "Epoch [41/50], Loss: 0.0068\n",
            "Epoch [42/50], Loss: 0.0129\n",
            "Epoch [43/50], Loss: 0.0177\n",
            "Epoch [44/50], Loss: 0.0243\n",
            "Epoch [45/50], Loss: 0.0326\n",
            "Epoch [46/50], Loss: 0.1133\n",
            "Epoch [47/50], Loss: 0.3102\n",
            "Epoch [48/50], Loss: 0.2755\n",
            "Epoch [49/50], Loss: 0.1595\n",
            "Epoch [50/50], Loss: 0.1506\n",
            "Epoch [1/50], Loss: 0.7805\n",
            "Epoch [2/50], Loss: 0.6064\n",
            "Epoch [3/50], Loss: 0.4658\n",
            "Epoch [4/50], Loss: 0.3602\n",
            "Epoch [5/50], Loss: 0.2905\n",
            "Epoch [6/50], Loss: 0.2449\n",
            "Epoch [7/50], Loss: 0.2176\n",
            "Epoch [8/50], Loss: 0.1956\n",
            "Epoch [9/50], Loss: 0.1757\n",
            "Epoch [10/50], Loss: 0.1509\n",
            "Epoch [11/50], Loss: 0.1415\n",
            "Epoch [12/50], Loss: 0.1313\n",
            "Epoch [13/50], Loss: 0.1262\n",
            "Epoch [14/50], Loss: 0.1068\n",
            "Epoch [15/50], Loss: 0.1024\n",
            "Epoch [16/50], Loss: 0.1004\n",
            "Epoch [17/50], Loss: 0.0966\n",
            "Epoch [18/50], Loss: 0.0912\n",
            "Epoch [19/50], Loss: 0.0823\n",
            "Epoch [20/50], Loss: 0.0745\n",
            "Epoch [21/50], Loss: 0.0724\n",
            "Epoch [22/50], Loss: 0.0736\n",
            "Epoch [23/50], Loss: 0.0638\n",
            "Epoch [24/50], Loss: 0.0630\n",
            "Epoch [25/50], Loss: 0.0599\n",
            "Epoch [26/50], Loss: 0.0551\n",
            "Epoch [27/50], Loss: 0.0558\n",
            "Epoch [28/50], Loss: 0.0564\n",
            "Epoch [29/50], Loss: 0.0544\n",
            "Epoch [30/50], Loss: 0.0493\n",
            "Epoch [31/50], Loss: 0.0457\n",
            "Epoch [32/50], Loss: 0.0434\n",
            "Epoch [33/50], Loss: 0.0426\n",
            "Epoch [34/50], Loss: 0.0433\n",
            "Epoch [35/50], Loss: 0.0402\n",
            "Epoch [36/50], Loss: 0.0424\n",
            "Epoch [37/50], Loss: 0.0408\n",
            "Epoch [38/50], Loss: 0.0374\n",
            "Epoch [39/50], Loss: 0.0384\n",
            "Epoch [40/50], Loss: 0.0376\n",
            "Epoch [41/50], Loss: 0.0380\n",
            "Epoch [42/50], Loss: 0.0356\n",
            "Epoch [43/50], Loss: 0.0324\n",
            "Epoch [44/50], Loss: 0.0367\n",
            "Epoch [45/50], Loss: 0.0628\n",
            "Epoch [46/50], Loss: 0.0348\n",
            "Epoch [47/50], Loss: 0.0299\n",
            "Epoch [48/50], Loss: 0.0284\n",
            "Epoch [49/50], Loss: 0.0275\n",
            "Epoch [50/50], Loss: 0.0289\n",
            "Epoch [1/50], Loss: 1.2892\n",
            "Epoch [2/50], Loss: 1.0010\n",
            "Epoch [3/50], Loss: 0.8473\n",
            "Epoch [4/50], Loss: 0.7673\n",
            "Epoch [5/50], Loss: 0.7279\n",
            "Epoch [6/50], Loss: 0.6911\n",
            "Epoch [7/50], Loss: 0.6737\n",
            "Epoch [8/50], Loss: 0.6534\n",
            "Epoch [9/50], Loss: 0.6367\n",
            "Epoch [10/50], Loss: 0.6238\n",
            "Epoch [11/50], Loss: 0.5942\n",
            "Epoch [12/50], Loss: 0.5861\n",
            "Epoch [13/50], Loss: 0.5598\n",
            "Epoch [14/50], Loss: 0.5437\n",
            "Epoch [15/50], Loss: 0.5245\n",
            "Epoch [16/50], Loss: 0.5075\n",
            "Epoch [17/50], Loss: 0.4861\n",
            "Epoch [18/50], Loss: 0.4766\n",
            "Epoch [19/50], Loss: 0.4563\n",
            "Epoch [20/50], Loss: 0.4451\n",
            "Epoch [21/50], Loss: 0.4353\n",
            "Epoch [22/50], Loss: 0.4125\n",
            "Epoch [23/50], Loss: 0.4020\n",
            "Epoch [24/50], Loss: 0.3907\n",
            "Epoch [25/50], Loss: 0.3872\n",
            "Epoch [26/50], Loss: 0.3726\n",
            "Epoch [27/50], Loss: 0.3725\n",
            "Epoch [28/50], Loss: 0.3582\n",
            "Epoch [29/50], Loss: 0.3490\n",
            "Epoch [30/50], Loss: 0.3394\n",
            "Epoch [31/50], Loss: 0.3285\n",
            "Epoch [32/50], Loss: 0.3236\n",
            "Epoch [33/50], Loss: 0.3143\n",
            "Epoch [34/50], Loss: 0.3148\n",
            "Epoch [35/50], Loss: 0.3010\n",
            "Epoch [36/50], Loss: 0.3000\n",
            "Epoch [37/50], Loss: 0.2944\n",
            "Epoch [38/50], Loss: 0.2872\n",
            "Epoch [39/50], Loss: 0.2856\n",
            "Epoch [40/50], Loss: 0.2717\n",
            "Epoch [41/50], Loss: 0.2680\n",
            "Epoch [42/50], Loss: 0.2698\n",
            "Epoch [43/50], Loss: 0.2602\n",
            "Epoch [44/50], Loss: 0.2571\n",
            "Epoch [45/50], Loss: 0.2476\n",
            "Epoch [46/50], Loss: 0.2465\n",
            "Epoch [47/50], Loss: 0.2444\n",
            "Epoch [48/50], Loss: 0.2467\n",
            "Epoch [49/50], Loss: 0.2339\n",
            "Epoch [50/50], Loss: 0.2315\n",
            "Epoch [1/50], Loss: 1.3590\n",
            "Epoch [2/50], Loss: 1.3219\n",
            "Epoch [3/50], Loss: 1.2847\n",
            "Epoch [4/50], Loss: 1.2532\n",
            "Epoch [5/50], Loss: 1.2202\n",
            "Epoch [6/50], Loss: 1.1901\n",
            "Epoch [7/50], Loss: 1.1579\n",
            "Epoch [8/50], Loss: 1.1322\n",
            "Epoch [9/50], Loss: 1.1036\n",
            "Epoch [10/50], Loss: 1.0754\n",
            "Epoch [11/50], Loss: 1.0527\n",
            "Epoch [12/50], Loss: 1.0247\n",
            "Epoch [13/50], Loss: 1.0028\n",
            "Epoch [14/50], Loss: 0.9849\n",
            "Epoch [15/50], Loss: 0.9647\n",
            "Epoch [16/50], Loss: 0.9382\n",
            "Epoch [17/50], Loss: 0.9230\n",
            "Epoch [18/50], Loss: 0.9076\n",
            "Epoch [19/50], Loss: 0.8919\n",
            "Epoch [20/50], Loss: 0.8853\n",
            "Epoch [21/50], Loss: 0.8646\n",
            "Epoch [22/50], Loss: 0.8476\n",
            "Epoch [23/50], Loss: 0.8383\n",
            "Epoch [24/50], Loss: 0.8370\n",
            "Epoch [25/50], Loss: 0.8177\n",
            "Epoch [26/50], Loss: 0.8105\n",
            "Epoch [27/50], Loss: 0.7958\n",
            "Epoch [28/50], Loss: 0.7934\n",
            "Epoch [29/50], Loss: 0.7839\n",
            "Epoch [30/50], Loss: 0.7789\n",
            "Epoch [31/50], Loss: 0.7859\n",
            "Epoch [32/50], Loss: 0.7720\n",
            "Epoch [33/50], Loss: 0.7767\n",
            "Epoch [34/50], Loss: 0.7490\n",
            "Epoch [35/50], Loss: 0.7587\n",
            "Epoch [36/50], Loss: 0.7518\n",
            "Epoch [37/50], Loss: 0.7408\n",
            "Epoch [38/50], Loss: 0.7375\n",
            "Epoch [39/50], Loss: 0.7372\n",
            "Epoch [40/50], Loss: 0.7245\n",
            "Epoch [41/50], Loss: 0.7424\n",
            "Epoch [42/50], Loss: 0.7191\n",
            "Epoch [43/50], Loss: 0.7342\n",
            "Epoch [44/50], Loss: 0.7232\n",
            "Epoch [45/50], Loss: 0.7161\n",
            "Epoch [46/50], Loss: 0.7019\n",
            "Epoch [47/50], Loss: 0.7056\n",
            "Epoch [48/50], Loss: 0.7109\n",
            "Epoch [49/50], Loss: 0.7124\n",
            "Epoch [50/50], Loss: 0.6966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 5: Comparing Batch Sizes\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "results_batch_sizes = []\n",
        "for batch_size in batch_sizes:\n",
        "    accuracy = train_and_evaluate(hidden_layers=[32], activation_fn='relu', learning_rate=0.01, batch_size=batch_size, epochs=50)\n",
        "    results_batch_sizes.append({'batch_size': batch_size, 'accuracy': accuracy})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqPVogT2m2Yt",
        "outputId": "889d01dc-8ece-4238-9d97-ba7961552d50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.7082\n",
            "Epoch [2/50], Loss: 0.4663\n",
            "Epoch [3/50], Loss: 0.3303\n",
            "Epoch [4/50], Loss: 0.2510\n",
            "Epoch [5/50], Loss: 0.2112\n",
            "Epoch [6/50], Loss: 0.1717\n",
            "Epoch [7/50], Loss: 0.1532\n",
            "Epoch [8/50], Loss: 0.1395\n",
            "Epoch [9/50], Loss: 0.1175\n",
            "Epoch [10/50], Loss: 0.1121\n",
            "Epoch [11/50], Loss: 0.0963\n",
            "Epoch [12/50], Loss: 0.0894\n",
            "Epoch [13/50], Loss: 0.0820\n",
            "Epoch [14/50], Loss: 0.0752\n",
            "Epoch [15/50], Loss: 0.0691\n",
            "Epoch [16/50], Loss: 0.0659\n",
            "Epoch [17/50], Loss: 0.0655\n",
            "Epoch [18/50], Loss: 0.0582\n",
            "Epoch [19/50], Loss: 0.0613\n",
            "Epoch [20/50], Loss: 0.0492\n",
            "Epoch [21/50], Loss: 0.0497\n",
            "Epoch [22/50], Loss: 0.0446\n",
            "Epoch [23/50], Loss: 0.0516\n",
            "Epoch [24/50], Loss: 0.0460\n",
            "Epoch [25/50], Loss: 0.0395\n",
            "Epoch [26/50], Loss: 0.0459\n",
            "Epoch [27/50], Loss: 0.0428\n",
            "Epoch [28/50], Loss: 0.0499\n",
            "Epoch [29/50], Loss: 0.0347\n",
            "Epoch [30/50], Loss: 0.0398\n",
            "Epoch [31/50], Loss: 0.0347\n",
            "Epoch [32/50], Loss: 0.0296\n",
            "Epoch [33/50], Loss: 0.0245\n",
            "Epoch [34/50], Loss: 0.0282\n",
            "Epoch [35/50], Loss: 0.0233\n",
            "Epoch [36/50], Loss: 0.0245\n",
            "Epoch [37/50], Loss: 0.0220\n",
            "Epoch [38/50], Loss: 0.0223\n",
            "Epoch [39/50], Loss: 0.0283\n",
            "Epoch [40/50], Loss: 0.0199\n",
            "Epoch [41/50], Loss: 0.0170\n",
            "Epoch [42/50], Loss: 0.0146\n",
            "Epoch [43/50], Loss: 0.0155\n",
            "Epoch [44/50], Loss: 0.0151\n",
            "Epoch [45/50], Loss: 0.0133\n",
            "Epoch [46/50], Loss: 0.0134\n",
            "Epoch [47/50], Loss: 0.0179\n",
            "Epoch [48/50], Loss: 0.0192\n",
            "Epoch [49/50], Loss: 0.0224\n",
            "Epoch [50/50], Loss: 0.0158\n",
            "Epoch [1/50], Loss: 0.7893\n",
            "Epoch [2/50], Loss: 0.5927\n",
            "Epoch [3/50], Loss: 0.4637\n",
            "Epoch [4/50], Loss: 0.3611\n",
            "Epoch [5/50], Loss: 0.2946\n",
            "Epoch [6/50], Loss: 0.2470\n",
            "Epoch [7/50], Loss: 0.2246\n",
            "Epoch [8/50], Loss: 0.1899\n",
            "Epoch [9/50], Loss: 0.1671\n",
            "Epoch [10/50], Loss: 0.1468\n",
            "Epoch [11/50], Loss: 0.1332\n",
            "Epoch [12/50], Loss: 0.1265\n",
            "Epoch [13/50], Loss: 0.1136\n",
            "Epoch [14/50], Loss: 0.1074\n",
            "Epoch [15/50], Loss: 0.1024\n",
            "Epoch [16/50], Loss: 0.0961\n",
            "Epoch [17/50], Loss: 0.0860\n",
            "Epoch [18/50], Loss: 0.0773\n",
            "Epoch [19/50], Loss: 0.0759\n",
            "Epoch [20/50], Loss: 0.0773\n",
            "Epoch [21/50], Loss: 0.0689\n",
            "Epoch [22/50], Loss: 0.0687\n",
            "Epoch [23/50], Loss: 0.0632\n",
            "Epoch [24/50], Loss: 0.0541\n",
            "Epoch [25/50], Loss: 0.0609\n",
            "Epoch [26/50], Loss: 0.0610\n",
            "Epoch [27/50], Loss: 0.0561\n",
            "Epoch [28/50], Loss: 0.0575\n",
            "Epoch [29/50], Loss: 0.0545\n",
            "Epoch [30/50], Loss: 0.0484\n",
            "Epoch [31/50], Loss: 0.0465\n",
            "Epoch [32/50], Loss: 0.0503\n",
            "Epoch [33/50], Loss: 0.0428\n",
            "Epoch [34/50], Loss: 0.0381\n",
            "Epoch [35/50], Loss: 0.0379\n",
            "Epoch [36/50], Loss: 0.0370\n",
            "Epoch [37/50], Loss: 0.0363\n",
            "Epoch [38/50], Loss: 0.0330\n",
            "Epoch [39/50], Loss: 0.0334\n",
            "Epoch [40/50], Loss: 0.0353\n",
            "Epoch [41/50], Loss: 0.0318\n",
            "Epoch [42/50], Loss: 0.0302\n",
            "Epoch [43/50], Loss: 0.0276\n",
            "Epoch [44/50], Loss: 0.0271\n",
            "Epoch [45/50], Loss: 0.0253\n",
            "Epoch [46/50], Loss: 0.0255\n",
            "Epoch [47/50], Loss: 0.0255\n",
            "Epoch [48/50], Loss: 0.0266\n",
            "Epoch [49/50], Loss: 0.0350\n",
            "Epoch [50/50], Loss: 0.0312\n",
            "Epoch [1/50], Loss: 0.8517\n",
            "Epoch [2/50], Loss: 0.6342\n",
            "Epoch [3/50], Loss: 0.5471\n",
            "Epoch [4/50], Loss: 0.4463\n",
            "Epoch [5/50], Loss: 0.3598\n",
            "Epoch [6/50], Loss: 0.3034\n",
            "Epoch [7/50], Loss: 0.2656\n",
            "Epoch [8/50], Loss: 0.2393\n",
            "Epoch [9/50], Loss: 0.2133\n",
            "Epoch [10/50], Loss: 0.1978\n",
            "Epoch [11/50], Loss: 0.1796\n",
            "Epoch [12/50], Loss: 0.1624\n",
            "Epoch [13/50], Loss: 0.1499\n",
            "Epoch [14/50], Loss: 0.1405\n",
            "Epoch [15/50], Loss: 0.1317\n",
            "Epoch [16/50], Loss: 0.1192\n",
            "Epoch [17/50], Loss: 0.1142\n",
            "Epoch [18/50], Loss: 0.1042\n",
            "Epoch [19/50], Loss: 0.1024\n",
            "Epoch [20/50], Loss: 0.0978\n",
            "Epoch [21/50], Loss: 0.0940\n",
            "Epoch [22/50], Loss: 0.0853\n",
            "Epoch [23/50], Loss: 0.0846\n",
            "Epoch [24/50], Loss: 0.0779\n",
            "Epoch [25/50], Loss: 0.0774\n",
            "Epoch [26/50], Loss: 0.0694\n",
            "Epoch [27/50], Loss: 0.0685\n",
            "Epoch [28/50], Loss: 0.0659\n",
            "Epoch [29/50], Loss: 0.0643\n",
            "Epoch [30/50], Loss: 0.0592\n",
            "Epoch [31/50], Loss: 0.0594\n",
            "Epoch [32/50], Loss: 0.0571\n",
            "Epoch [33/50], Loss: 0.0554\n",
            "Epoch [34/50], Loss: 0.0505\n",
            "Epoch [35/50], Loss: 0.0490\n",
            "Epoch [36/50], Loss: 0.0496\n",
            "Epoch [37/50], Loss: 0.0495\n",
            "Epoch [38/50], Loss: 0.0469\n",
            "Epoch [39/50], Loss: 0.0467\n",
            "Epoch [40/50], Loss: 0.0453\n",
            "Epoch [41/50], Loss: 0.0453\n",
            "Epoch [42/50], Loss: 0.0411\n",
            "Epoch [43/50], Loss: 0.0375\n",
            "Epoch [44/50], Loss: 0.0361\n",
            "Epoch [45/50], Loss: 0.0360\n",
            "Epoch [46/50], Loss: 0.0360\n",
            "Epoch [47/50], Loss: 0.0423\n",
            "Epoch [48/50], Loss: 0.0372\n",
            "Epoch [49/50], Loss: 0.0331\n",
            "Epoch [50/50], Loss: 0.0321\n",
            "Epoch [1/50], Loss: 1.0336\n",
            "Epoch [2/50], Loss: 0.7375\n",
            "Epoch [3/50], Loss: 0.6682\n",
            "Epoch [4/50], Loss: 0.6278\n",
            "Epoch [5/50], Loss: 0.5838\n",
            "Epoch [6/50], Loss: 0.5313\n",
            "Epoch [7/50], Loss: 0.4767\n",
            "Epoch [8/50], Loss: 0.4211\n",
            "Epoch [9/50], Loss: 0.3727\n",
            "Epoch [10/50], Loss: 0.3350\n",
            "Epoch [11/50], Loss: 0.3025\n",
            "Epoch [12/50], Loss: 0.2766\n",
            "Epoch [13/50], Loss: 0.2537\n",
            "Epoch [14/50], Loss: 0.2353\n",
            "Epoch [15/50], Loss: 0.2187\n",
            "Epoch [16/50], Loss: 0.2021\n",
            "Epoch [17/50], Loss: 0.1881\n",
            "Epoch [18/50], Loss: 0.1738\n",
            "Epoch [19/50], Loss: 0.1634\n",
            "Epoch [20/50], Loss: 0.1565\n",
            "Epoch [21/50], Loss: 0.1447\n",
            "Epoch [22/50], Loss: 0.1454\n",
            "Epoch [23/50], Loss: 0.1312\n",
            "Epoch [24/50], Loss: 0.1235\n",
            "Epoch [25/50], Loss: 0.1179\n",
            "Epoch [26/50], Loss: 0.1152\n",
            "Epoch [27/50], Loss: 0.1061\n",
            "Epoch [28/50], Loss: 0.1045\n",
            "Epoch [29/50], Loss: 0.0982\n",
            "Epoch [30/50], Loss: 0.0954\n",
            "Epoch [31/50], Loss: 0.0913\n",
            "Epoch [32/50], Loss: 0.0871\n",
            "Epoch [33/50], Loss: 0.0887\n",
            "Epoch [34/50], Loss: 0.0829\n",
            "Epoch [35/50], Loss: 0.0833\n",
            "Epoch [36/50], Loss: 0.0784\n",
            "Epoch [37/50], Loss: 0.0752\n",
            "Epoch [38/50], Loss: 0.0723\n",
            "Epoch [39/50], Loss: 0.0711\n",
            "Epoch [40/50], Loss: 0.0695\n",
            "Epoch [41/50], Loss: 0.0656\n",
            "Epoch [42/50], Loss: 0.0642\n",
            "Epoch [43/50], Loss: 0.0616\n",
            "Epoch [44/50], Loss: 0.0622\n",
            "Epoch [45/50], Loss: 0.0599\n",
            "Epoch [46/50], Loss: 0.0592\n",
            "Epoch [47/50], Loss: 0.0562\n",
            "Epoch [48/50], Loss: 0.0550\n",
            "Epoch [49/50], Loss: 0.0556\n",
            "Epoch [50/50], Loss: 0.0542\n",
            "Epoch [1/50], Loss: 1.1077\n",
            "Epoch [2/50], Loss: 0.8029\n",
            "Epoch [3/50], Loss: 0.7453\n",
            "Epoch [4/50], Loss: 0.6974\n",
            "Epoch [5/50], Loss: 0.6654\n",
            "Epoch [6/50], Loss: 0.6403\n",
            "Epoch [7/50], Loss: 0.6177\n",
            "Epoch [8/50], Loss: 0.5769\n",
            "Epoch [9/50], Loss: 0.5635\n",
            "Epoch [10/50], Loss: 0.5534\n",
            "Epoch [11/50], Loss: 0.4989\n",
            "Epoch [12/50], Loss: 0.4844\n",
            "Epoch [13/50], Loss: 0.4484\n",
            "Epoch [14/50], Loss: 0.4400\n",
            "Epoch [15/50], Loss: 0.4079\n",
            "Epoch [16/50], Loss: 0.4008\n",
            "Epoch [17/50], Loss: 0.3687\n",
            "Epoch [18/50], Loss: 0.3397\n",
            "Epoch [19/50], Loss: 0.3326\n",
            "Epoch [20/50], Loss: 0.3114\n",
            "Epoch [21/50], Loss: 0.2948\n",
            "Epoch [22/50], Loss: 0.2876\n",
            "Epoch [23/50], Loss: 0.2688\n",
            "Epoch [24/50], Loss: 0.2585\n",
            "Epoch [25/50], Loss: 0.2527\n",
            "Epoch [26/50], Loss: 0.2321\n",
            "Epoch [27/50], Loss: 0.2280\n",
            "Epoch [28/50], Loss: 0.2208\n",
            "Epoch [29/50], Loss: 0.2112\n",
            "Epoch [30/50], Loss: 0.2036\n",
            "Epoch [31/50], Loss: 0.1937\n",
            "Epoch [32/50], Loss: 0.1844\n",
            "Epoch [33/50], Loss: 0.1835\n",
            "Epoch [34/50], Loss: 0.1713\n",
            "Epoch [35/50], Loss: 0.1633\n",
            "Epoch [36/50], Loss: 0.1616\n",
            "Epoch [37/50], Loss: 0.1578\n",
            "Epoch [38/50], Loss: 0.1517\n",
            "Epoch [39/50], Loss: 0.1449\n",
            "Epoch [40/50], Loss: 0.1393\n",
            "Epoch [41/50], Loss: 0.1383\n",
            "Epoch [42/50], Loss: 0.1303\n",
            "Epoch [43/50], Loss: 0.1290\n",
            "Epoch [44/50], Loss: 0.1225\n",
            "Epoch [45/50], Loss: 0.1222\n",
            "Epoch [46/50], Loss: 0.1161\n",
            "Epoch [47/50], Loss: 0.1109\n",
            "Epoch [48/50], Loss: 0.1094\n",
            "Epoch [49/50], Loss: 0.1042\n",
            "Epoch [50/50], Loss: 0.1082\n",
            "Epoch [1/50], Loss: 1.1974\n",
            "Epoch [2/50], Loss: 0.9966\n",
            "Epoch [3/50], Loss: 0.8695\n",
            "Epoch [4/50], Loss: 0.7853\n",
            "Epoch [5/50], Loss: 0.7498\n",
            "Epoch [6/50], Loss: 0.7225\n",
            "Epoch [7/50], Loss: 0.6999\n",
            "Epoch [8/50], Loss: 0.6834\n",
            "Epoch [9/50], Loss: 0.6590\n",
            "Epoch [10/50], Loss: 0.6479\n",
            "Epoch [11/50], Loss: 0.6342\n",
            "Epoch [12/50], Loss: 0.6140\n",
            "Epoch [13/50], Loss: 0.6047\n",
            "Epoch [14/50], Loss: 0.5847\n",
            "Epoch [15/50], Loss: 0.5689\n",
            "Epoch [16/50], Loss: 0.5491\n",
            "Epoch [17/50], Loss: 0.5307\n",
            "Epoch [18/50], Loss: 0.5130\n",
            "Epoch [19/50], Loss: 0.4944\n",
            "Epoch [20/50], Loss: 0.4768\n",
            "Epoch [21/50], Loss: 0.4598\n",
            "Epoch [22/50], Loss: 0.4371\n",
            "Epoch [23/50], Loss: 0.4201\n",
            "Epoch [24/50], Loss: 0.4046\n",
            "Epoch [25/50], Loss: 0.3884\n",
            "Epoch [26/50], Loss: 0.3759\n",
            "Epoch [27/50], Loss: 0.3581\n",
            "Epoch [28/50], Loss: 0.3488\n",
            "Epoch [29/50], Loss: 0.3351\n",
            "Epoch [30/50], Loss: 0.3227\n",
            "Epoch [31/50], Loss: 0.3136\n",
            "Epoch [32/50], Loss: 0.3016\n",
            "Epoch [33/50], Loss: 0.2916\n",
            "Epoch [34/50], Loss: 0.2796\n",
            "Epoch [35/50], Loss: 0.2720\n",
            "Epoch [36/50], Loss: 0.2652\n",
            "Epoch [37/50], Loss: 0.2563\n",
            "Epoch [38/50], Loss: 0.2479\n",
            "Epoch [39/50], Loss: 0.2416\n",
            "Epoch [40/50], Loss: 0.2294\n",
            "Epoch [41/50], Loss: 0.2256\n",
            "Epoch [42/50], Loss: 0.2215\n",
            "Epoch [43/50], Loss: 0.2142\n",
            "Epoch [44/50], Loss: 0.2069\n",
            "Epoch [45/50], Loss: 0.2018\n",
            "Epoch [46/50], Loss: 0.1998\n",
            "Epoch [47/50], Loss: 0.1907\n",
            "Epoch [48/50], Loss: 0.1833\n",
            "Epoch [49/50], Loss: 0.1806\n",
            "Epoch [50/50], Loss: 0.1748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate and save results\n",
        "hidden_layers_df = pd.DataFrame(results_hidden_layers)\n",
        "activation_functions_df = pd.DataFrame(results_activation_functions)\n",
        "epochs_df = pd.DataFrame(results_epochs)\n",
        "learning_rates_df = pd.DataFrame(results_learning_rates)\n",
        "batch_sizes_df = pd.DataFrame(results_batch_sizes)\n",
        "\n",
        "# Save each result separately\n",
        "hidden_layers_df.to_csv('mlp_hidden_layers_results.csv', index=False)\n",
        "activation_functions_df.to_csv('mlp_activation_functions_results.csv', index=False)\n",
        "epochs_df.to_csv('mlp_epochs_results.csv', index=False)\n",
        "learning_rates_df.to_csv('mlp_learning_rates_results.csv', index=False)\n",
        "batch_sizes_df.to_csv('mlp_batch_sizes_results.csv', index=False)\n",
        "\n",
        "print(\"Results saved for each experiment.\")"
      ],
      "metadata": {
        "id": "ttHfyZ5zoE6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e93d83fc-2092-42a1-872b-930619120cee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved for each experiment.\n"
          ]
        }
      ]
    }
  ]
}